#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ—©æŠ¥å‰ç«¯åº”ç”¨ - åç«¯APIæœåŠ¡
æä¾›æ—©æŠ¥æ•°æ®çš„REST APIæ¥å£
"""

import os
import re
import json
from datetime import datetime
from typing import List, Dict, Optional
from flask import Flask, jsonify, request, render_template
from flask_cors import CORS
import markdown

app = Flask(__name__)
CORS(app)

# é…ç½®
DOCS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'docs')
PAGE_SIZE = 10

class NewspaperService:
    """æ—©æŠ¥æ•°æ®æœåŠ¡"""

    def __init__(self, docs_dir: str):
        self.docs_dir = docs_dir
        self._cache = {}
        self._last_load_time = None

    def _parse_filename(self, filename: str) -> Dict:
        """è§£ææ–‡ä»¶åè·å–ä¿¡æ¯"""
        # æ–‡ä»¶åæ ¼å¼: BVå·_æ—¥æœŸ_AIæ—©æŠ¥.md
        match = re.match(r'([^_]+)_(\d{4}-\d{2}-\d{2})_AIæ—©æŠ¥\.md', filename)
        if match:
            return {
                'bv_id': match.group(1),
                'date': match.group(2),
                'filename': filename
            }
        return None

    def _parse_markdown_file(self, filepath: str) -> Dict:
        """è§£æmarkdownæ–‡ä»¶å†…å®¹"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–æ ‡é¢˜
            title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
            title = title_match.group(1) if title_match else 'æœªçŸ¥æ ‡é¢˜'

            # æå–å‘å¸ƒæ—¥æœŸ
            date_match = re.search(r'\*\*ğŸ“… å‘å¸ƒæ—¥æœŸï¼š\*\* (\d{4}-\d{2}-\d{2})', content)
            publish_date = date_match.group(1) if date_match else None

            # æå–BVå·
            bv_match = re.search(r'\*\*ğŸ¬ BVå·ï¼š\*\* ([^\n]+)', content)
            bv_id = bv_match.group(1) if bv_match else None

            # æå–æ•´ç†æ—¶é—´
            time_match = re.search(r'\*\*ğŸ“ æ•´ç†æ—¶é—´ï¼š\*\* ([^\n]+)', content)
            organize_time = time_match.group(1) if time_match else None

            # æå–èµ„è®¯æ•°é‡
            count_match = re.search(r'\*\*ğŸ“Š èµ„è®¯æ•°é‡ï¼š\*\* (\d+)', content)
            news_count = int(count_match.group(1)) if count_match else 0

            # æå–æ¦‚è§ˆ
            overview_match = re.search(r'## ğŸ“‹ æœ¬æœŸæ¦‚è§ˆ\n\n(.+?)\n\n---', content, re.DOTALL)
            overview = overview_match.group(1).strip() if overview_match else ''

            return {
                'title': title,
                'publish_date': publish_date,
                'bv_id': bv_id,
                'organize_time': organize_time,
                'news_count': news_count,
                'overview': overview,
                'content': content,
                'html_content': markdown.markdown(
                    content,
                    extensions=[
                        'extra',
                        'codehilite',
                        'tables',
                        'toc',
                        'fenced_code',
                        'nl2br',
                        'attr_list',
                        'def_list',
                        'footnotes',
                        'admonition'
                    ],
                    extension_configs={
                        'codehilite': {
                            'css_class': 'highlight',
                            'use_pygments': True
                        }
                    }
                )
            }
        except Exception as e:
            print(f"è§£ææ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None

    def _load_newspapers(self) -> List[Dict]:
        """åŠ è½½æ‰€æœ‰æ—©æŠ¥æ•°æ®"""
        current_time = datetime.now()

        # å¦‚æœç¼“å­˜å­˜åœ¨ä¸”æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›
        if self._cache and self._last_load_time and \
           (current_time - self._last_load_time).seconds < 300:  # 5åˆ†é’Ÿç¼“å­˜
            return self._cache['newspapers']

        newspapers = []

        if not os.path.exists(self.docs_dir):
            print(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {self.docs_dir}")
            return newspapers

        # éå†docsç›®å½•ä¸‹çš„æ‰€æœ‰markdownæ–‡ä»¶
        for filename in os.listdir(self.docs_dir):
            if filename.endswith('.md'):
                file_info = self._parse_filename(filename)
                if file_info:
                    filepath = os.path.join(self.docs_dir, filename)
                    newspaper_data = self._parse_markdown_file(filepath)

                    if newspaper_data:
                        # åˆå¹¶æ–‡ä»¶ä¿¡æ¯å’Œè§£æå†…å®¹
                        newspaper_data.update(file_info)
                        newspapers.append(newspaper_data)

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        newspapers.sort(key=lambda x: x.get('publish_date', ''), reverse=True)

        # æ›´æ–°ç¼“å­˜
        self._cache = {
            'newspapers': newspapers,
            'total_count': len(newspapers)
        }
        self._last_load_time = current_time

        return newspapers

    def get_newspapers(self, page: int = 1, page_size: int = PAGE_SIZE) -> Dict:
        """è·å–æ—©æŠ¥åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰"""
        newspapers = self._load_newspapers()
        total_count = len(newspapers)

        # è®¡ç®—åˆ†é¡µ
        start_index = (page - 1) * page_size
        end_index = start_index + page_size
        page_newspapers = newspapers[start_index:end_index]

        # è®¡ç®—æ€»é¡µæ•°
        total_pages = (total_count + page_size - 1) // page_size

        return {
            'newspapers': page_newspapers,
            'pagination': {
                'current_page': page,
                'page_size': page_size,
                'total_count': total_count,
                'total_pages': total_pages,
                'has_next': page < total_pages,
                'has_prev': page > 1
            }
        }

    def get_newspaper_by_filename(self, filename: str) -> Optional[Dict]:
        """æ ¹æ®æ–‡ä»¶åè·å–å•ä¸ªæ—©æŠ¥è¯¦æƒ…"""
        newspapers = self._load_newspapers()
        for newspaper in newspapers:
            if newspaper.get('filename') == filename:
                return newspaper
        return None

# åˆ›å»ºæ—©æŠ¥æœåŠ¡å®ä¾‹
newspaper_service = NewspaperService(DOCS_DIR)

@app.route('/')
def index():
    """ä¸»é¡µ - è¿”å›å‰ç«¯é¡µé¢"""
    return render_template('index.html')

@app.route('/api/newspapers')
def get_newspapers():
    """è·å–æ—©æŠ¥åˆ—è¡¨API"""
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', PAGE_SIZE))

        result = newspaper_service.get_newspapers(page, page_size)
        return jsonify({
            'success': True,
            'data': result
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/newspapers/<filename>')
def get_newspaper_detail(filename):
    """è·å–æ—©æŠ¥è¯¦æƒ…API"""
    try:
        newspaper = newspaper_service.get_newspaper_by_filename(filename)
        if newspaper:
            return jsonify({
                'success': True,
                'data': newspaper
            })
        else:
            return jsonify({
                'success': False,
                'error': 'æ—©æŠ¥ä¸å­˜åœ¨'
            }), 404
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/refresh')
def refresh_cache():
    """åˆ·æ–°ç¼“å­˜API"""
    try:
        # æ¸…ç©ºç¼“å­˜å¼ºåˆ¶é‡æ–°åŠ è½½
        newspaper_service._cache = {}
        newspaper_service._last_load_time = None

        newspapers = newspaper_service._load_newspapers()
        return jsonify({
            'success': True,
            'data': {
                'total_count': len(newspapers),
                'message': 'ç¼“å­˜åˆ·æ–°æˆåŠŸ'
            }
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    print(f"æ–‡æ¡£ç›®å½•: {DOCS_DIR}")
    print("å¯åŠ¨AIæ—©æŠ¥å‰ç«¯æœåŠ¡...")
    app.run(debug=True, host='0.0.0.0', port=5001)
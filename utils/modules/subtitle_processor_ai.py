"""
AIé©±åŠ¨çš„å­—å¹•å¤„ç†æ¨¡å—
ä½¿ç”¨ OpenAI API æ™ºèƒ½æç‚¼æ–°é—»è¦ç‚¹ã€ç”Ÿæˆæ¦‚è§ˆå’Œæå–æ¥æºé“¾æ¥
"""

import os
import re
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

from ..logger import get_logger
from .bilibili_api import BilibiliAPI
from .content_formatter import ContentFormatter

load_dotenv()
LLM_MODEL = os.getenv("OPENAI_MODEL")
MAX_TOKENS = int(os.getenv("OPENAI_MAX_TOKENS", "8192"))

class AISubtitleProcessor:
    """AIé©±åŠ¨çš„å­—å¹•æ™ºèƒ½å¤„ç†å™¨"""

    def __init__(self, video_dir: Optional[Path] = None):
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—å™¨
        self.logger = get_logger()

        # ä½¿ç”¨ OpenAI API
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=90.0
        )

        # åˆå§‹åŒ– Bilibili API
        self.bilibili_api = BilibiliAPI({})

        # è®¾ç½®è§†é¢‘æ•°æ®ä¿å­˜ç›®å½•
        self.video_dir = video_dir

        # åˆå§‹åŒ–å†…å®¹æ ¼å¼åŒ–å™¨
        self.content_formatter = ContentFormatter()

    def process(self, subtitle_data: List[Dict], video_info: Dict, speech_texts: List[str] = None) -> Dict:
        """
        ä½¿ç”¨AIå¤„ç†å­—å¹•æ•°æ®ï¼Œç”Ÿæˆç»“æ„åŒ–çš„æ–°é—»æŠ¥å‘Š

        Args:
            subtitle_data: å­—å¹•åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« from, to, content
            video_info: è§†é¢‘ä¿¡æ¯ï¼ŒåŒ…å« bvid, title, desc ç­‰
            speech_texts: è¯­éŸ³è½¬æ–‡å­—ç»“æœï¼ˆå½“æ²¡æœ‰å­—å¹•æ—¶å¿…é¡»æä¾›ï¼‰

        Returns:
            å¤„ç†åçš„ç»“æ„åŒ–æ•°æ®
        """
        # æå–è§†é¢‘æè¿°ä¸­çš„é“¾æ¥
        desc_links = self._extract_links_from_desc(video_info.get('desc', ''))
        video_desc = video_info.get('desc', '').strip()
        video_title = video_info.get('title', '')

        # ç”Ÿæˆæ—¥æœŸç›®å½•ï¼ˆç”¨äºä¿å­˜è¯„è®ºæ•°æ®ï¼‰
        date_dir = datetime.fromtimestamp(video_info.get('pubdate', 0)).strftime('%Y%m%d')

        # æ˜ç¡®äº”ç§å¤„ç†åœºæ™¯ï¼š
        # åœºæ™¯1: æœ‰å­—å¹• - ç›´æ¥ä½¿ç”¨å­—å¹•ç”Ÿæˆæ—©æŠ¥
        if subtitle_data and len(subtitle_data) > 0:
            self.logger.info("Using subtitles to extract news...")
            # 1. åˆå¹¶å­—å¹•æ–‡æœ¬
            full_text = self._merge_subtitles(subtitle_data)
            # 2. ä½¿ç”¨AIæç‚¼æ–°é—»å†…å®¹
            news_items = self._ai_extract_news(full_text, subtitle_data, desc_links, video_title)

        # åœºæ™¯2ã€3ã€4ã€5: æ²¡æœ‰å­—å¹•
        else:
            # åœºæ™¯2: æœ‰ç®€ä»‹ä¸”æœ‰è¯­éŸ³è½¬æ–‡å­— - ä¼˜å…ˆç»“åˆç”Ÿæˆï¼ˆè´¨é‡æœ€é«˜ï¼‰
            if video_desc and len(video_desc) >= 30 and speech_texts:
                self.logger.info("Combining video description with speech-to-text for enhanced news extraction...")
                news_items = self._extract_news_from_description_and_speech(video_desc, speech_texts, desc_links, video_title)

            # åœºæ™¯3: æœ‰ç®€ä»‹ä½†æ— è¯­éŸ³è½¬æ–‡å­— - ä»…ä½¿ç”¨ç®€ä»‹ï¼ˆæ— éœ€è¯­éŸ³è½¬å†™èƒ½åŠ›ï¼‰
            elif video_desc and len(video_desc) >= 30:
                self.logger.info("No subtitles available, using video description to extract news...")
                news_items = self._extract_news_from_description(video_desc, desc_links, video_title)

            # åœºæ™¯4: ç®€ä»‹å¤ªçŸ­ä½†æœ‰è¯­éŸ³è½¬æ–‡å­— - å°è¯•è·å–è¯„è®ºå¹¶ç»“åˆè¯­éŸ³è½¬æ–‡å­—
            elif speech_texts:
                self.logger.info("Video description too short or empty, attempting to fetch comments and combine with speech-to-text...")
                news_items = self._extract_news_from_speech_and_comments(speech_texts, desc_links, video_title, video_info, date_dir)

            # åœºæ™¯5: æ— ç®€ä»‹ä¸”æ— è¯­éŸ³è½¬æ–‡å­— - å°è¯•ä»…ä½¿ç”¨è¯„è®ºï¼ˆå¦‚æœæœ‰ï¼‰
            else:
                self.logger.info("No subtitles, description, or speech-to-text available, attempting to extract news from comments only...")
                news_items = self._extract_news_from_comments_only(desc_links, video_title, video_info, date_dir)

        # 3. ç”Ÿæˆæ¦‚è§ˆ
        overview_text = self._ai_generate_overview(news_items, video_info, video_title)

        # 4. æ„å»ºæœ€ç»ˆç»“æ„
        overview = {
            'summary': overview_text,
            'total_news': len(news_items),
            'video_title': video_info.get('title', ''),
            'bvid': video_info.get('bvid', ''),
            'publish_date': datetime.fromtimestamp(video_info.get('pubdate', 0)).strftime('%Y-%m-%d'),
            'processed_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        return {
            'overview': overview,
            'news_items': news_items,
            'raw_subtitles': subtitle_data if subtitle_data else [],
            'speech_texts': speech_texts if speech_texts else [],
            'video_info': video_info
        }

    def _merge_subtitles(self, subtitles: List[Dict]) -> str:
        """åˆå¹¶å­—å¹•ä¸ºå®Œæ•´æ–‡æœ¬"""
        return ' '.join([s['content'] for s in subtitles])

    def _extract_json_from_response(self, result_text: str) -> dict:
        """
        ä»APIå“åº”ä¸­æå–JSONæ•°æ®

        Args:
            result_text: APIè¿”å›çš„æ–‡æœ¬å†…å®¹

        Returns:
            è§£æåçš„JSONå¯¹è±¡

        Raises:
            json.JSONDecodeError: JSONè§£æå¤±è´¥æ—¶æŠ›å‡º
        """
        # æå–JSONï¼ˆå»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°ï¼‰
        if result_text.startswith('```'):
            result_text = result_text.split('```')[1]
            if result_text.startswith('json'):
                result_text = result_text[4:]

        return json.loads(result_text)

    def _extract_links_from_desc(self, desc: str) -> List[Dict[str, any]]:
        """ä»è§†é¢‘æè¿°ä¸­æå–é“¾æ¥ï¼ˆå¸¦æ ‡é¢˜å’Œæ—¶é—´æˆ³ï¼‰"""
        links_with_context = []

        # æŒ‰è¡Œåˆ†å‰²æè¿°
        lines = desc.split('\n')
        current_title = None
        current_time = None

        for line in lines:
            line = line.strip()

            # åŒ¹é…æ ‡é¢˜è¡Œï¼šâ¬›ï¸ æ ‡é¢˜: æ—¶é—´
            title_match = re.match(r'â¬›ï¸\s+(.+?):\s+(\d+:\d+)', line)
            if title_match:
                current_title = title_match.group(1).strip()
                current_time = title_match.group(2).strip()
                continue

            # åŒ¹é…é“¾æ¥è¡Œï¼šğŸ”— https://...
            link_match = re.match(r'ğŸ”—\s+(https?://[^\s]+)', line)
            if link_match and current_title:
                links_with_context.append({
                    'title': current_title,
                    'time': current_time,
                    'url': link_match.group(1).strip()
                })

        return links_with_context

    def _ai_extract_news(self, full_text: str, subtitles: List[Dict], desc_links: List[Dict], video_title: str = "") -> List[Dict]:
        """ä½¿ç”¨AIæç‚¼æ–°é—»æ¡ç›®"""

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ä»ä»¥ä¸‹AIæ—©æŠ¥çš„å­—å¹•æ–‡æœ¬ä¸­ï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

å­—å¹•æ–‡æœ¬ï¼š
{full_text}

è¦æ±‚ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œæ³¨æ„è¯†åˆ«æ ‡é¢˜å¯¹åº”çš„æ–°é—»å†…å®¹
2. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»
3. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
4. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - æ ¸å¿ƒäº‹ä»¶æè¿°ï¼ˆä»€ä¹ˆå…¬å¸/äº§å“å‘å¸ƒ/æ›´æ–°äº†ä»€ä¹ˆï¼‰
   - å…³é”®åŠŸèƒ½ã€ç‰¹æ€§ã€æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†è¯´æ˜
   - ä½¿ç”¨åœºæ™¯ã€åº”ç”¨ä»·å€¼æˆ–è¡Œä¸šå½±å“
   - ä¿ç•™å­—å¹•ä¸­æåˆ°çš„æ‰€æœ‰å…·ä½“æ•°æ®ã€ç‰ˆæœ¬å·ã€æ—¶é—´ç‚¹ã€æŠ€æœ¯æœ¯è¯­
5. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
6. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”ï¼Œæä¾›å……åˆ†ä¿¡æ¯é‡
7. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹çš„è¯¦ç»†ç¨‹åº¦

å†…å®¹å†™ä½œè¦æ±‚ï¼š
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œä¸è¦æ¦‚æ‹¬æ€§æè¿°
- å°†å­—å¹•ä¸­çš„æŠ€æœ¯ç»†èŠ‚å®Œæ•´ä¿ç•™å¹¶å±•å¼€è¯´æ˜
- å¤šç”¨"åŠŸèƒ½åŒ…æ‹¬"ã€"ç‰¹ç‚¹æ˜¯"ã€"æ”¯æŒ"ç­‰è¯æ±‡æ¥å±•å¼€å†…å®¹
- é¿å…"æ­¤å¤–"ã€"åŒæ—¶"ç­‰ç”Ÿç¡¬è¿æ¥è¯ï¼Œæ”¹ç”¨è‡ªç„¶è¡”æ¥
- å°½å¯èƒ½è¯¦ç»†ï¼Œä½†ä¿æŒå†…å®¹çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹ï¼ˆ150-300å­—ï¼‰",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            return news_items

        except Exception as e:
            self.logger.error(f"AIæå–å¤±è´¥: {e}")
            # é™çº§ä¸ºç®€å•æå–
            return self._simple_extract_news(subtitles)

    def _match_links_for_news(self, news: Dict, desc_links: List[Dict]) -> List[str]:
        """å°è¯•ä¸ºæ–°é—»åŒ¹é…ç›¸å…³é“¾æ¥"""
        matched_links = []

        # ç­–ç•¥ï¼šåŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦åŒ¹é…
        news_title = news.get('title', '').lower()
        news_content = news.get('content', '').lower()
        news_entities = [e.lower() for e in news.get('entities', [])]

        for link_item in desc_links:
            desc_title = link_item['title'].lower()
            url = link_item['url']

            # è®¡ç®—ç›¸ä¼¼åº¦
            score = 0

            # 1. å®ä½“åŒ¹é…
            for entity in news_entities:
                if entity in desc_title:
                    score += 3

            # 2. æ ‡é¢˜å…³é”®è¯åŒ¹é…
            news_words = set(news_title.split())
            desc_words = set(desc_title.split())
            common_words = news_words & desc_words
            score += len(common_words)

            # 3. å†…å®¹å…³é”®è¯åŒ¹é…
            if any(word in news_content for word in desc_title.split()):
                score += 1

            if score >= 2:  # é˜ˆå€¼
                url = url.replace('http://','https://') # é¿å…å‡ºç°httpé“¾æ¥
                matched_links.append(url)

        return matched_links[:3]  # æœ€å¤š3ä¸ªé“¾æ¥


    def _ai_generate_overview(self, news_items: List[Dict], video_info: Dict, video_title: str = "") -> str:
        """ä½¿ç”¨AIç”Ÿæˆæœ¬æœŸæ¦‚è§ˆ"""

        # æ„å»ºæ–°é—»åˆ—è¡¨
        news_list = '\n'.join([f"{i}. {item['title']}" for i, item in enumerate(news_items, 1)])

        prompt = f"""ä½ æ˜¯AIèµ„è®¯ç¼–è¾‘ï¼Œè¯·ä¸ºè¿™æœŸAIæ—©æŠ¥å†™ä¸€æ®µç®€æ´çš„æ¦‚è§ˆï¼ˆ60-120å­—ï¼‰ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_info.get('title', '')}
æ–°é—»åˆ—è¡¨ï¼š
{news_list}

è¦æ±‚ï¼š
1. è¯†åˆ«è§†é¢‘æ ‡é¢˜æŒ‡å‘çš„é‡ç‚¹æ–°é—»ï¼Œçªå‡ºå…¶é‡è¦æ€§
2. ç”¨2-3å¥è¯æ¦‚æ‹¬æœ¬æœŸæ ¸å¿ƒå†…å®¹
3. é‡ç‚¹çªå‡ºè§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘æ–°é—»çš„å…³é”®è¯å’Œæ ¸å¿ƒä¿¡æ¯
4. ç®€æ´ã€ä¿¡æ¯å¯†åº¦é«˜ï¼Œä¸è¦å†—ä½™ä¿®é¥°
5. é¿å…ä½¿ç”¨"æœ¬æœŸ"ã€"ä»Šå¤©"ã€"æ­¤å¤–"ã€"åŒæ—¶"ç­‰è¯
6. ç›´æ¥é™ˆè¿°äº‹å®ï¼Œä¸è¦è¯„è®ºæ€§è¯­è¨€
7. ç¡®ä¿æ¦‚è§ˆé‡ç‚¹çªå‡ºä¸è§†é¢‘æ ‡é¢˜ç›¸å…³çš„é‡è¦å†…å®¹

åªè¿”å›æ¦‚è§ˆæ–‡æœ¬ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=MAX_TOKENS
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            self.logger.error(f"AIç”Ÿæˆæ¦‚è§ˆå¤±è´¥: {e}")
            return f"æœ¬æœŸAIæ—©æŠ¥å…±åŒ…å« {len(news_items)} æ¡èµ„è®¯ï¼Œæ¶µç›–AIé¢†åŸŸçš„æœ€æ–°åŠ¨æ€ã€‚"

    def _extract_news_from_description_and_speech(self, description: str, speech_texts: List[str], desc_links: List[Dict], video_title: str = "") -> List[Dict]:
        """
        ç»“åˆè§†é¢‘ç®€ä»‹å’Œè¯­éŸ³è½¬æ–‡å­—ç»“æœæå–æ–°é—»ï¼ˆåœºæ™¯2ï¼šæœ‰ç®€ä»‹+è¯­éŸ³è½¬æ–‡å­—ï¼‰

        Args:
            description: è§†é¢‘ç®€ä»‹æ–‡æœ¬
            speech_texts: è¯­éŸ³è½¬æ–‡å­—ç»“æœåˆ—è¡¨
            desc_links: ä»ç®€ä»‹ä¸­æå–çš„é“¾æ¥åˆ—è¡¨
            video_title: è§†é¢‘æ ‡é¢˜

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if not speech_texts:
            self.logger.warning("Speech recognition result is empty, cannot extract news")
            return []

        # åˆå¹¶æ‰€æœ‰å£°é“çš„æ–‡æœ¬
        full_speech_text = ' '.join(speech_texts)

        self.logger.info(f"Combining description and speech-to-text for news extraction, desc length: {len(description)}, speech length: {len(full_speech_text)}")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ç»“åˆä»¥ä¸‹è§†é¢‘æ ‡é¢˜ã€è§†é¢‘ç®€ä»‹å’Œè¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

è§†é¢‘ç®€ä»‹ï¼š
{description}

è¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼š
{full_speech_text}

é‡è¦è¯´æ˜ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œéœ€è¦æ³¨æ„è¯†åˆ«å¯¹åº”çš„æ–°é—»å†…å®¹
2. è§†é¢‘ç®€ä»‹é€šå¸¸æä¾›äº†æ–°é—»çš„æ ¸å¿ƒè¦ç‚¹å’Œç»“æ„ï¼Œä½†å¯èƒ½ä¸å¤Ÿè¯¦ç»†
3. è¯­éŸ³è½¬æ–‡å­—åŒ…å«äº†è¯¦ç»†çš„è®²è§£å†…å®¹ï¼Œä½†å¯èƒ½å­˜åœ¨ä¸“æœ‰åè¯è½¬å†™é”™è¯¯
4. è¯·ç»“åˆä¸‰è€…çš„ä¼˜åŠ¿ï¼šç”¨æ ‡é¢˜è¯†åˆ«é‡ç‚¹æ–°é—»ï¼Œç”¨ç®€ä»‹ç¡®å®šæ–°é—»ç»“æ„å’Œè¦ç‚¹ï¼Œç”¨è¯­éŸ³è½¬æ–‡å­—è¡¥å……è¯¦ç»†ä¿¡æ¯

å¤„ç†ç­–ç•¥ï¼š
1. è¯†åˆ«è§†é¢‘æ ‡é¢˜æŒ‡å‘çš„é‡ç‚¹æ–°é—»å†…å®¹
2. ä¼˜å…ˆä»è§†é¢‘ç®€ä»‹ä¸­è¯†åˆ«æ–°é—»æ¡ç›®çš„ç»“æ„å’Œæ ‡é¢˜
3. ä»è¯­éŸ³è½¬æ–‡å­—ä¸­æå–è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚ã€åŠŸèƒ½æè¿°å’Œå…·ä½“æ•°æ®
4. ä¿®æ­£è¯­éŸ³è½¬æ–‡å­—ä¸­å¯èƒ½é”™è¯¯çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“æœ‰åè¯
5. è¡¥å……ç®€ä»‹ä¸­å¯èƒ½ç¼ºå¤±çš„é‡è¦ç»†èŠ‚
6. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹è¯¦ç»†ç¨‹åº¦

è¦æ±‚ï¼š
1. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»
2. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
3. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - æ ¸å¿ƒäº‹ä»¶æè¿°ï¼ˆä»€ä¹ˆå…¬å¸/äº§å“å‘å¸ƒ/æ›´æ–°äº†ä»€ä¹ˆï¼‰
   - å…³é”®åŠŸèƒ½ã€ç‰¹æ€§ã€æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†è¯´æ˜
   - ä½¿ç”¨åœºæ™¯ã€åº”ç”¨ä»·å€¼æˆ–è¡Œä¸šå½±å“
   - ä¿ç•™è¯­éŸ³è½¬æ–‡å­—ä¸­çš„æ‰€æœ‰å…·ä½“æ•°æ®ã€ç‰ˆæœ¬å·ã€æ—¶é—´ç‚¹
4. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
5. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”ï¼Œæä¾›å……åˆ†ä¿¡æ¯é‡
6. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹çš„è¯¦ç»†ç¨‹åº¦

å†…å®¹å†™ä½œè¦æ±‚ï¼š
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œä¸è¦æ¦‚æ‹¬æ€§æè¿°
- å°†è¯­éŸ³è½¬æ–‡å­—ä¸­çš„æŠ€æœ¯ç»†èŠ‚å®Œæ•´ä¿ç•™å¹¶å±•å¼€è¯´æ˜
- ä¿®æ­£æ˜æ˜¾çš„è¯­éŸ³è½¬å†™é”™è¯¯ï¼ˆå®ä½“åç§°ã€ä¸“æœ‰åè¯ç­‰ï¼‰
- å¤šç”¨"åŠŸèƒ½åŒ…æ‹¬"ã€"ç‰¹ç‚¹æ˜¯"ã€"æ”¯æŒ"ç­‰è¯æ±‡æ¥å±•å¼€å†…å®¹
- é¿å…ç”Ÿç¡¬è¿æ¥è¯ï¼Œæ”¹ç”¨è‡ªç„¶è¡”æ¥
- å°½å¯èƒ½è¯¦ç»†ï¼Œä½†ä¿æŒå†…å®¹çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹ï¼ˆ150-300å­—ï¼‰",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            self.logger.info(f"Extracted {len(news_items)} news items from combined description and speech-to-text")
            return news_items

        except Exception as e:
            self.logger.error(f"Failed to extract news from combined description and speech-to-text: {e}")
            # å¦‚æœç»“åˆå¤„ç†å¤±è´¥ï¼Œé™çº§ä¸ºä»…ä½¿ç”¨è¯­éŸ³è½¬æ–‡å­—
            self.logger.info("Falling back to speech-to-text only...")
            return self._extract_news_from_speech_text(speech_texts, desc_links, video_title)

    def _extract_news_from_description(self, description: str, desc_links: List[Dict], video_title: str = "") -> List[Dict]:
        """
        ä»è§†é¢‘ç®€ä»‹ä¸­æå–æ–°é—»ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼Œå½“æ²¡æœ‰å­—å¹•æ—¶ä½¿ç”¨ï¼‰

        Args:
            description: è§†é¢‘ç®€ä»‹æ–‡æœ¬
            desc_links: ä»ç®€ä»‹ä¸­æå–çš„é“¾æ¥åˆ—è¡¨
            video_title: è§†é¢‘æ ‡é¢˜

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if not description or len(description.strip()) < 30:
            self.logger.warning("Video description too short to extract news")
            return []

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ç»“åˆè§†é¢‘æ ‡é¢˜å’Œè§†é¢‘ç®€ä»‹ï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

è§†é¢‘ç®€ä»‹ï¼š
{description}

è¦æ±‚ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œæ³¨æ„åœ¨ç®€ä»‹ä¸­è¯†åˆ«å¯¹åº”çš„æ–°é—»å†…å®¹
2. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»ï¼ˆç®€ä»‹ä¸­é€šå¸¸ä¼šåˆ—å‡ºå¤šæ¡æ–°é—»ï¼‰
3. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
4. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - æ ¸å¿ƒäº‹ä»¶æè¿°ï¼ˆä»€ä¹ˆå…¬å¸/äº§å“å‘å¸ƒ/æ›´æ–°äº†ä»€ä¹ˆï¼‰
   - ä»ç®€ä»‹ä¸­èƒ½æ¨æ–­å‡ºçš„å…³é”®åŠŸèƒ½ã€ç‰¹æ€§
   - å¯èƒ½çš„åº”ç”¨ä»·å€¼æˆ–å½±å“
   - ä¿ç•™ç®€ä»‹ä¸­çš„å…·ä½“æ•°æ®ã€ç‰ˆæœ¬å·ã€æ—¶é—´ç‚¹ã€æŠ€æœ¯æœ¯è¯­
5. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
6. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”
7. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹çš„è¯¦ç»†ç¨‹åº¦

æ³¨æ„ï¼š
- ç®€ä»‹é€šå¸¸ä¼šç”¨"â¬›ï¸"æˆ–æ•°å­—æ ‡æ³¨æ¯æ¡æ–°é—»
- ç®€ä»‹å¯èƒ½æ¯”å­—å¹•ç®€çŸ­ï¼Œè¯·æ ¹æ®æœ‰é™ä¿¡æ¯åˆç†è¡¥å……å†…å®¹
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œä½†ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            self.logger.info(f"Extracted {len(news_items)} news items from video description")
            return news_items

        except Exception as e:
            self.logger.error(f"Failed to extract news from description: {e}")
            # å¦‚æœAIæå–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []

    def _extract_news_from_speech_text(self, speech_texts: List[str], desc_links: List[Dict], video_title: str = "") -> List[Dict]:
        """
        ä»è¯­éŸ³è½¬æ–‡å­—ç»“æœä¸­æå–æ–°é—»ï¼ˆå…œåº•æ–¹æ¡ˆï¼Œå½“è§†é¢‘ç®€ä»‹ä¸ºç©ºæ—¶ä½¿ç”¨ï¼‰

        Args:
            speech_texts: è¯­éŸ³è¯†åˆ«ç»“æœæ–‡æœ¬åˆ—è¡¨
            desc_links: ä»ç®€ä»‹ä¸­æå–çš„é“¾æ¥åˆ—è¡¨ï¼ˆé€šå¸¸ä¸ºç©ºï¼‰
            video_title: è§†é¢‘æ ‡é¢˜

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if not speech_texts:
            self.logger.warning("Speech recognition result is empty, cannot extract news")
            return []

        # åˆå¹¶æ‰€æœ‰å£°é“çš„æ–‡æœ¬
        full_text = ' '.join(speech_texts)

        self.logger.info(f"Starting news extraction from speech-to-text, text length: {len(full_text)} characters")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ç»“åˆè§†é¢‘æ ‡é¢˜å’Œè¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

è¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼š
{full_text}

é‡è¦è¯´æ˜ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œéœ€è¦æ³¨æ„è¯†åˆ«å¯¹åº”çš„æ–°é—»å†…å®¹
2. è¯­éŸ³è½¬æ–‡å­—å†…å®¹å› è¯­éŸ³è½¬å†™å¯èƒ½å­˜åœ¨å¤±çœŸï¼Œéœ€è¦æ ¹æ®ä¸“ä¸šçŸ¥è¯†ä¿®æ­£ä¸ºæ­£ç¡®çš„è®¡ç®—æœºã€å¤§æ¨¡å‹è¡Œä¸šä¸“æœ‰åè¯

è¦æ±‚ï¼š
1. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»
2. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
3. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - æ ¸å¿ƒäº‹ä»¶æè¿°ï¼ˆä»€ä¹ˆå…¬å¸/äº§å“å‘å¸ƒ/æ›´æ–°äº†ä»€ä¹ˆï¼‰
   - å…³é”®åŠŸèƒ½ã€ç‰¹æ€§ã€æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†è¯´æ˜
   - ä½¿ç”¨åœºæ™¯ã€åº”ç”¨ä»·å€¼æˆ–è¡Œä¸šå½±å“
   - ä¿®æ­£è¯­éŸ³è½¬å†™ä¸­å¯èƒ½é”™è¯¯çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“æœ‰åè¯
4. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
5. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”ï¼Œæä¾›å……åˆ†ä¿¡æ¯é‡
6. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹çš„è¯¦ç»†ç¨‹åº¦

å†…å®¹å†™ä½œè¦æ±‚ï¼š
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œä¸è¦æ¦‚æ‹¬æ€§æè¿°
- å°†è¯­éŸ³è½¬æ–‡å­—ä¸­çš„æŠ€æœ¯ç»†èŠ‚å®Œæ•´ä¿ç•™å¹¶å±•å¼€è¯´æ˜
- ä¿®æ­£æ˜æ˜¾çš„è¯­éŸ³è½¬å†™é”™è¯¯ï¼ˆå¦‚"GPT"å¯èƒ½è¢«è½¬å†™ä¸º"GPTT"ç­‰ï¼‰
- å¤šç”¨"åŠŸèƒ½åŒ…æ‹¬"ã€"ç‰¹ç‚¹æ˜¯"ã€"æ”¯æŒ"ç­‰è¯æ±‡æ¥å±•å¼€å†…å®¹
- é¿å…"æ­¤å¤–"ã€"åŒæ—¶"ç­‰ç”Ÿç¡¬è¿æ¥è¯ï¼Œæ”¹ç”¨è‡ªç„¶è¡”æ¥
- å°½å¯èƒ½è¯¦ç»†ï¼Œä½†ä¿æŒå†…å®¹çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹ï¼ˆ150-300å­—ï¼‰",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥ï¼ˆè™½ç„¶é€šå¸¸ä¸ºç©ºï¼‰
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            self.logger.info(f"Extracted {len(news_items)} news items from speech-to-text result")
            return news_items

        except Exception as e:
            self.logger.error(f"Failed to extract news from speech-to-text: {e}")
            # å¦‚æœAIæå–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []

    def format_markdown(self, processed_data: Dict) -> str:
        """
        å°†å¤„ç†åçš„æ•°æ®æ ¼å¼åŒ–ä¸ºç²¾ç¾çš„ Markdown

        Args:
            processed_data: process() è¿”å›çš„ç»“æ„åŒ–æ•°æ®

        Returns:
            Markdown æ ¼å¼çš„æ–‡æœ¬
        """
        return self.content_formatter.format_markdown(processed_data)

    def generate_email_html(self, processed_data: Dict) -> str:
        """
        ç”Ÿæˆç²¾ç¾çš„HTMLé‚®ä»¶å†…å®¹

        Args:
            processed_data: process() è¿”å›çš„ç»“æ„åŒ–æ•°æ®

        Returns:
            HTML æ ¼å¼çš„é‚®ä»¶å†…å®¹
        """
        return self.content_formatter.generate_email_html(processed_data)

    def save_comments_output(self, comments: List[Dict], date_dir: str) -> bool:
        """
        ä¿å­˜è¯„è®ºæ•°æ®åˆ°comments_output.txtæ–‡ä»¶

        Args:
            comments: è¯„è®ºæ•°æ®åˆ—è¡¨
            date_dir: æ—¥æœŸç›®å½•å (YYYYMMDD)

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        if not comments:
            self.logger.warning("No comments to save")
            return False

        if not self.video_dir:
            self.logger.warning("video_dir not configured, cannot save comments")
            return False

        target_dir = self.video_dir / date_dir
        comments_output_file = target_dir / "comments_output.txt"

        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            target_dir.mkdir(parents=True, exist_ok=True)

            self.logger.info(f"Saving comments output to: {comments_output_file}")

            # å‡†å¤‡ä¿å­˜çš„å†…å®¹
            content = []
            content.append("=" * 60)
            content.append("è§†é¢‘è¯„è®ºæ•°æ®")
            content.append("=" * 60)
            content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            content.append(f"è¯„è®ºæ•°é‡: {len(comments)}")
            content.append("")

            for i, comment in enumerate(comments, 1):
                content.append(f"è¯„è®º {i}:")
                content.append("-" * 20)
                content.append(f"ä½œè€…: {comment.get('author', 'Unknown')}")
                content.append(f"å†…å®¹: {comment.get('content', '')}")
                if 'like' in comment:
                    content.append(f"ç‚¹èµæ•°: {comment['like']}")
                content.append("")

            # å†™å…¥æ–‡ä»¶
            with open(comments_output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(content))

            self.logger.info(f"Comments output saved successfully: {comments_output_file}")
            return True

        except Exception as e:
            self.logger.error(f"Failed to save comments output: {e}")
            return False

    def _get_uploader_comments_with_retry(self, video_info: Dict, date_dir: str = None, max_retries: int = 10, retry_interval: int = 120) -> List[Dict]:
        """
        è·å–UPä¸»ç›¸å…³çš„è¯„è®ºï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼Œä¸“é—¨å¯»æ‰¾åŒ…å«æ—¶é—´æˆ³æ ¼å¼çš„è¯„è®º

        Args:
            video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
            date_dir: æ—¥æœŸç›®å½•å (YYYYMMDD)ï¼Œç”¨äºä¿å­˜è¯„è®ºæ•°æ®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤10æ¬¡
            retry_interval: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤120ç§’ï¼ˆ2åˆ†é’Ÿï¼‰

        Returns:
            åŒ…å«æ—¶é—´æˆ³çš„è¯„è®ºå†…å®¹åˆ—è¡¨
        """
        bvid = video_info.get('bvid', '')
        if not bvid:
            self.logger.warning("No BV ID found in video info, cannot fetch comments")
            return []

        # é¦–æ¬¡å°è¯•è·å–è¯„è®º
        self.logger.info(f"Starting to fetch uploader comments for video {bvid}")
        comments = self._get_uploader_comments(video_info, date_dir)

        if comments:
            self.logger.info("Successfully fetched uploader comments on first attempt")
            return comments

        # å¦‚æœé¦–æ¬¡è·å–å¤±è´¥ï¼Œå¼€å§‹é‡è¯•
        self.logger.info(f"No uploader comments found on first attempt, starting retry process (max {max_retries} retries)")

        for retry_count in range(1, max_retries + 1):
            self.logger.info(f"Retry attempt {retry_count}/{max_retries} for video {bvid}")

            # ç­‰å¾…æŒ‡å®šé—´éš”
            if retry_count > 1:
                self.logger.info(f"Waiting {retry_interval} seconds before next retry...")
                time.sleep(retry_interval)

            # å°è¯•è·å–è¯„è®º
            comments = self._get_uploader_comments(video_info, date_dir)

            if comments:
                self.logger.info(f"Successfully fetched uploader comments after {retry_count} retries")
                return comments

            self.logger.warning(f"Retry attempt {retry_count} failed to fetch comments")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        self.logger.error(f"Failed to fetch uploader comments after {max_retries} retries (total time: {max_retries * retry_interval / 60:.1f} minutes)")
        return []

    def _get_uploader_comments(self, video_info: Dict, date_dir: str = None) -> List[Dict]:
        """
        è·å–UPä¸»ç›¸å…³çš„è¯„è®ºï¼Œä¸“é—¨å¯»æ‰¾åŒ…å«æ—¶é—´æˆ³æ ¼å¼çš„è¯„è®ºï¼ˆå•æ¬¡å°è¯•ï¼Œä¸åŒ…å«é‡è¯•é€»è¾‘ï¼‰

        Args:
            video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
            date_dir: æ—¥æœŸç›®å½•å (YYYYMMDD)ï¼Œç”¨äºä¿å­˜è¯„è®ºæ•°æ®

        Returns:
            åŒ…å«æ—¶é—´æˆ³çš„è¯„è®ºå†…å®¹åˆ—è¡¨
        """
        try:
            bvid = video_info.get('bvid', '')
            if not bvid:
                self.logger.warning("No BV ID found in video info, cannot fetch comments")
                return []

            # è·å–æ‰€æœ‰UPä¸»ç›¸å…³çš„è¯„è®º
            comments = self.bilibili_api.get_all_uploader_related_comments(bvid)

            if not comments:
                self.logger.info("No uploader-related comments found")
                return []

            # ç­›é€‰åŒ…å«æ—¶é—´æˆ³æ ¼å¼çš„è¯„è®º
            timestamp_comments = []
            for comment in comments:
                content = comment.get('content', '')
                if self._contains_timestamp_format(content):
                    timestamp_comments.append(comment)
                    self.logger.info(f"Found timestamp comment from {comment['author']}")

            if timestamp_comments:
                self.logger.info(f"Successfully found {len(timestamp_comments)} comments with timestamp format")

                # ä¿å­˜è¯„è®ºæ•°æ®åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†date_dirå‚æ•°ï¼‰
                if date_dir and self.video_dir:
                    self.save_comments_output(timestamp_comments, date_dir)

                return timestamp_comments
            else:
                self.logger.info("No comments with timestamp format found, skipping comment processing")
                return []

        except Exception as e:
            self.logger.error(f"Failed to fetch uploader comments: {e}")
            return []

    def _contains_timestamp_format(self, content: str) -> bool:
        """
        æ£€æŸ¥è¯„è®ºå†…å®¹æ˜¯å¦åŒ…å«æ—¶é—´æˆ³æ ¼å¼ï¼ˆå¦‚ "Intro: 00:00", "Outro: 05:53"ï¼‰

        Args:
            content: è¯„è®ºå†…å®¹

        Returns:
            æ˜¯å¦åŒ…å«æ—¶é—´æˆ³æ ¼å¼
        """
        import re

        # æ£€æŸ¥æ˜¯å¦åŒ…å« "Intro: XX:XX" æ ¼å¼
        intro_pattern = r'Intro:\s*\d{1,2}:\d{2}'
        if re.search(intro_pattern, content, re.IGNORECASE):
            return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å« "Outro: XX:XX" æ ¼å¼
        outro_pattern = r'Outro:\s*\d{1,2}:\d{2}'
        if re.search(outro_pattern, content, re.IGNORECASE):
            return True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–æ—¶é—´æˆ³æ ¼å¼ï¼ˆå¦‚ "å…¬å¸å: XX:XX"ï¼‰
        timestamp_pattern = r':\s*\d{1,2}:\d{2}'
        lines = content.split('\n')
        timestamp_lines = 0

        for line in lines:
            line = line.strip()
            # å¦‚æœä¸€è¡ŒåŒ…å«æ—¶é—´æˆ³æ ¼å¼ï¼Œè®¡æ•°
            if re.search(timestamp_pattern, line):
                timestamp_lines += 1

        # å¦‚æœæœ‰å¤šè¡ŒåŒ…å«æ—¶é—´æˆ³æ ¼å¼ï¼Œå¯èƒ½æ˜¯æ—©æŠ¥å†…å®¹
        return timestamp_lines >= 3

    def _extract_news_from_speech_and_comments(self, speech_texts: List[str], desc_links: List[Dict], video_title: str = "", video_info: Dict = None, date_dir: str = None) -> List[Dict]:
        """
        ç»“åˆè¯­éŸ³è½¬æ–‡å­—å’ŒUPä¸»è¯„è®ºæå–æ–°é—»ï¼ˆåœºæ™¯4ï¼šç®€ä»‹å¤ªçŸ­ä½†æœ‰è¯­éŸ³è½¬æ–‡å­—ï¼‰

        Args:
            speech_texts: è¯­éŸ³è½¬æ–‡å­—ç»“æœåˆ—è¡¨
            desc_links: ä»ç®€ä»‹ä¸­æå–çš„é“¾æ¥åˆ—è¡¨ï¼ˆé€šå¸¸ä¸ºç©ºï¼‰
            video_title: è§†é¢‘æ ‡é¢˜
            video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
            date_dir: æ—¥æœŸç›®å½•å (YYYYMMDD)ï¼Œç”¨äºä¿å­˜è¯„è®ºæ•°æ®

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        if not speech_texts:
            self.logger.warning("Speech recognition result is empty, cannot extract news")
            return []

        # è·å–UPä¸»è¯„è®ºï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        comments = self._get_uploader_comments_with_retry(video_info, date_dir) if video_info else []

        # åˆå¹¶æ‰€æœ‰å£°é“çš„æ–‡æœ¬
        full_speech_text = ' '.join(speech_texts)

        # åˆå¹¶è¯„è®ºå†…å®¹
        comments_text = ''
        if comments:
            comments_list = []
            for comment in comments:
                if comment.get('content'):
                    comments_list.append(comment.get('content', ''))
            comments_text = ' '.join(comments_list)

        self.logger.info(f"Extracting news from speech and comments, speech length: {len(full_speech_text)}, comments length: {len(comments_text)}")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ç»“åˆè§†é¢‘æ ‡é¢˜ã€è¯­éŸ³è½¬æ–‡å­—å†…å®¹å’ŒUPä¸»è¯„è®ºï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

è¯­éŸ³è½¬æ–‡å­—å†…å®¹ï¼š
{full_speech_text}

UPä¸»è¯„è®ºå†…å®¹ï¼š
{comments_text if comments_text else "æ— UPä¸»è¯„è®º"}

é‡è¦è¯´æ˜ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œéœ€è¦æ³¨æ„è¯†åˆ«å¯¹åº”çš„æ–°é—»å†…å®¹
2. è¯­éŸ³è½¬æ–‡å­—å†…å®¹å› è¯­éŸ³è½¬å†™å¯èƒ½å­˜åœ¨å¤±çœŸï¼Œéœ€è¦æ ¹æ®ä¸“ä¸šçŸ¥è¯†ä¿®æ­£
3. UPä¸»è¯„è®ºé€šå¸¸åŒ…å«é‡è¦çš„è¡¥å……ä¿¡æ¯ã€ä¿®æ­£è¯´æ˜æˆ–è¯¦ç»†çš„æ—¶é—´æˆ³å†…å®¹
4. ç‰¹åˆ«æ³¨æ„è¯„è®ºä¸­çš„æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆå¦‚"Intro: 00:00"ã€"Google ä¸Šçº¿...: 00:10"ç­‰ï¼‰ï¼Œè¿™äº›å¾€å¾€æ˜¯æ–°é—»æ¡ç›®çš„å‡†ç¡®æ—¶é—´ç‚¹
5. å¦‚æœè¯„è®ºä¸­æœ‰æ—¶é—´æˆ³æ ¼å¼çš„å†…å®¹ï¼Œè¿™æ˜¯æœ€æœ‰ä»·å€¼çš„æ–°é—»ç»“æ„ä¿¡æ¯

å¤„ç†ç­–ç•¥ï¼š
1. ä¼˜å…ˆä»UPä¸»è¯„è®ºä¸­è¯†åˆ«æ–°é—»æ¡ç›®ç»“æ„ï¼ˆç‰¹åˆ«æ˜¯æ—¶é—´æˆ³æ ¼å¼ï¼‰
2. ç”¨è¯„è®ºå†…å®¹æ¥ä¿®æ­£å’Œè¡¥å……è¯­éŸ³è½¬æ–‡å­—ä¸­çš„ä¿¡æ¯
3. è¯†åˆ«è§†é¢‘æ ‡é¢˜æŒ‡å‘çš„é‡ç‚¹æ–°é—»å†…å®¹
4. ä»è¯­éŸ³è½¬æ–‡å­—ä¸­æå–è¯¦ç»†çš„æŠ€æœ¯ç»†èŠ‚ã€åŠŸèƒ½æè¿°å’Œå…·ä½“æ•°æ®
5. ä¿®æ­£è¯­éŸ³è½¬æ–‡å­—ä¸­å¯èƒ½é”™è¯¯çš„æŠ€æœ¯æœ¯è¯­å’Œä¸“æœ‰åè¯

è¦æ±‚ï¼š
1. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»
2. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
3. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - æ ¸å¿ƒäº‹ä»¶æè¿°ï¼ˆä»€ä¹ˆå…¬å¸/äº§å“å‘å¸ƒ/æ›´æ–°äº†ä»€ä¹ˆï¼‰
   - å…³é”®åŠŸèƒ½ã€ç‰¹æ€§ã€æŠ€æœ¯ç»†èŠ‚çš„è¯¦ç»†è¯´æ˜
   - ä½¿ç”¨åœºæ™¯ã€åº”ç”¨ä»·å€¼æˆ–è¡Œä¸šå½±å“
   - ä¿ç•™è¯­éŸ³è½¬æ–‡å­—å’Œè¯„è®ºä¸­çš„æ‰€æœ‰å…·ä½“æ•°æ®ã€ç‰ˆæœ¬å·ã€æ—¶é—´ç‚¹
4. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
5. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”ï¼Œæä¾›å……åˆ†ä¿¡æ¯é‡
6. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»ï¼Œé€‚å½“å¢åŠ å…¶å†…å®¹è¯¦ç»†ç¨‹åº¦

å†…å®¹å†™ä½œè¦æ±‚ï¼š
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œä¸è¦æ¦‚æ‹¬æ€§æè¿°
- å°†è¯­éŸ³è½¬æ–‡å­—å’Œè¯„è®ºä¸­çš„æŠ€æœ¯ç»†èŠ‚å®Œæ•´ä¿ç•™å¹¶å±•å¼€è¯´æ˜
- ä¿®æ­£æ˜æ˜¾çš„è¯­éŸ³è½¬å†™é”™è¯¯ï¼ˆå®ä½“åç§°ã€ä¸“æœ‰åè¯ç­‰ï¼‰
- å¤šç”¨"åŠŸèƒ½åŒ…æ‹¬"ã€"ç‰¹ç‚¹æ˜¯"ã€"æ”¯æŒ"ç­‰è¯æ±‡æ¥å±•å¼€å†…å®¹
- é¿å…ç”Ÿç¡¬è¿æ¥è¯ï¼Œæ”¹ç”¨è‡ªç„¶è¡”æ¥
- å°½å¯èƒ½è¯¦ç»†ï¼Œä½†ä¿æŒå†…å®¹çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹ï¼ˆ150-300å­—ï¼‰",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            self.logger.info(f"Extracted {len(news_items)} news items from speech and comments")
            return news_items

        except Exception as e:
            self.logger.error(f"Failed to extract news from speech and comments: {e}")
            # å¦‚æœç»“åˆå¤„ç†å¤±è´¥ï¼Œé™çº§ä¸ºä»…ä½¿ç”¨è¯­éŸ³è½¬æ–‡å­—
            self.logger.info("Falling back to speech-to-text only...")
            return self._extract_news_from_speech_text(speech_texts, desc_links, video_title)

    def _extract_news_from_comments_only(self, desc_links: List[Dict], video_title: str = "", video_info: Dict = None, date_dir: str = None) -> List[Dict]:
        """
        ä»…ä»UPä¸»è¯„è®ºä¸­æå–æ–°é—»ï¼ˆåœºæ™¯5ï¼šæ— ç®€ä»‹ä¸”æ— è¯­éŸ³è½¬æ–‡å­—ï¼‰

        Args:
            desc_links: ä»ç®€ä»‹ä¸­æå–çš„é“¾æ¥åˆ—è¡¨ï¼ˆé€šå¸¸ä¸ºç©ºï¼‰
            video_title: è§†é¢‘æ ‡é¢˜
            video_info: è§†é¢‘ä¿¡æ¯å­—å…¸
            date_dir: æ—¥æœŸç›®å½•å (YYYYMMDD)ï¼Œç”¨äºä¿å­˜è¯„è®ºæ•°æ®

        Returns:
            æ–°é—»åˆ—è¡¨
        """
        # è·å–UPä¸»è¯„è®ºï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        comments = self._get_uploader_comments_with_retry(video_info, date_dir) if video_info else []

        if not comments:
            self.logger.warning("No uploader comments available, cannot extract news")
            return []

        # åˆå¹¶è¯„è®ºå†…å®¹
        comments_text = ''
        comments_list = []
        for comment in comments:
            if comment.get('content'):
                comments_list.append(comment.get('content', ''))
        comments_text = ' '.join(comments_list)

        self.logger.info(f"Extracting news from comments only, total comments: {len(comments)}, text length: {len(comments_text)}")

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIèµ„è®¯ç¼–è¾‘ã€‚è¯·ç»“åˆè§†é¢‘æ ‡é¢˜å’ŒUPä¸»è¯„è®ºï¼Œæç‚¼å‡ºç»“æ„åŒ–çš„æ–°é—»æ¡ç›®ã€‚

è§†é¢‘æ ‡é¢˜ï¼š{video_title}

UPä¸»è¯„è®ºå†…å®¹ï¼š
{comments_text}

é‡è¦è¯´æ˜ï¼š
1. è§†é¢‘æ ‡é¢˜é€šå¸¸æŒ‡å‘æœ¬æœŸæœ€é‡è¦çš„æ–°é—»ï¼Œéœ€è¦æ³¨æ„è¯†åˆ«å¯¹åº”çš„æ–°é—»å†…å®¹
2. UPä¸»è¯„è®ºæ˜¯å”¯ä¸€çš„ä¿¡æ¯æ¥æºï¼Œéœ€è¦å……åˆ†åˆ©ç”¨è¯„è®ºä¸­çš„ä¿¡æ¯
3. ç‰¹åˆ«æ³¨æ„è¯„è®ºä¸­çš„æ—¶é—´æˆ³ä¿¡æ¯ï¼ˆå¦‚"Intro: 00:00"ã€"Google ä¸Šçº¿...: 00:10"ç­‰ï¼‰ï¼Œè¿™äº›å¾€å¾€æ˜¯æ–°é—»æ¡ç›®çš„å‡†ç¡®ç»“æ„
4. è¯„è®ºä¸­çš„æ—¶é—´æˆ³æ ¼å¼å†…å®¹æ˜¯æœ€æœ‰ä»·å€¼çš„æ–°é—»ç»“æ„ä¿¡æ¯
5. å¯èƒ½éœ€è¦æ ¹æ®æœ‰é™çš„è¯„è®ºä¿¡æ¯è¿›è¡Œåˆç†çš„å†…å®¹æ‰©å±•

å¤„ç†ç­–ç•¥ï¼š
1. ä¼˜å…ˆä»è¯„è®ºä¸­è¯†åˆ«æ–°é—»æ¡ç›®ç»“æ„ï¼ˆç‰¹åˆ«æ˜¯æ—¶é—´æˆ³æ ¼å¼ï¼‰
2. è¯†åˆ«è§†é¢‘æ ‡é¢˜æŒ‡å‘çš„é‡ç‚¹æ–°é—»å†…å®¹
3. æ ¹æ®è¯„è®ºä¸­çš„ä¿¡æ¯æ¨æ–­æ–°é—»çš„è¯¦ç»†å†…å®¹
4. å¦‚æœè¯„è®ºä¿¡æ¯æœ‰é™ï¼Œéœ€è¦åŸºäºä¸“ä¸šèƒŒæ™¯è¿›è¡Œåˆç†çš„å†…å®¹è¡¥å……
5. ä¿æŒè¯„è®ºä¸­å·²æœ‰çš„å…·ä½“æ•°æ®å’Œäº‹å®

è¦æ±‚ï¼š
1. è¯†åˆ«å¹¶æå–æ¯ä¸€æ¡ç‹¬ç«‹çš„AIæ–°é—»
2. ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆä¸€ä¸ªç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-25å­—ï¼Œç®€æ´æ˜äº†ï¼‰
3. å†™ä¸€æ®µè¯¦ç»†çš„æ–°é—»æŠ¥é“ï¼Œå°½å¯èƒ½è¯¦ç»†åœ°åŒ…å«ï¼š
   - åŸºäºè¯„è®ºä¿¡æ¯æ¨æ–­çš„æ ¸å¿ƒäº‹ä»¶æè¿°
   - ä»è¯„è®ºä¸­èƒ½æå–æˆ–åˆç†æ¨æ–­çš„åŠŸèƒ½ã€ç‰¹æ€§è¯´æ˜
   - å¯èƒ½çš„åº”ç”¨ä»·å€¼æˆ–è¡Œä¸šå½±å“
   - ä¿ç•™è¯„è®ºä¸­çš„æ‰€æœ‰å…·ä½“æ•°æ®ã€ç‰ˆæœ¬å·ã€æ—¶é—´ç‚¹
4. æå–ç›¸å…³çš„å…¬å¸/äº§å“/æŠ€æœ¯åç§°ï¼ˆ2-3ä¸ªä¸»è¦å®ä½“ï¼‰
5. ä¿æŒä¸“ä¸šå®¢è§‚çš„è¯­æ°”
6. é‡ç‚¹å…³æ³¨è§†é¢‘æ ‡é¢˜æ‰€æŒ‡å‘çš„æ–°é—»

æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœè¯„è®ºä¿¡æ¯è¾ƒä¸ºç®€çŸ­ï¼Œéœ€è¦åŸºäºAIé¢†åŸŸçŸ¥è¯†è¿›è¡Œåˆç†çš„å†…å®¹æ‰©å±•
- ä¸è¦ç¼–é€ ä¸è¯„è®ºä¿¡æ¯æ˜æ˜¾çŸ›ç›¾çš„å†…å®¹
- è¯¦ç»†å±•å¼€æ¯ä¸ªè¦ç‚¹ï¼Œæä¾›å……åˆ†çš„ä¿¡æ¯é‡
- ä¿æŒå†…å®¹çš„å¯è¯»æ€§å’Œä¸“ä¸šæ€§

è¾“å‡ºJSONæ ¼å¼ï¼š
{{
  "news": [
    {{
      "title": "æ–°é—»æ ‡é¢˜",
      "content": "è¯¦ç»†æ–°é—»å†…å®¹ï¼ˆ150-300å­—ï¼‰",
      "entities": ["å…¬å¸/äº§å“å"],
      "category": "äº§å“å‘å¸ƒ|æŠ€æœ¯æ›´æ–°|è¡Œä¸šåŠ¨æ€|å…¶ä»–"
    }}
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

        try:
            response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4,  # ç¨å¾®æé«˜åˆ›é€ æ€§æ¥è¡¥å……ä¿¡æ¯
                max_tokens=MAX_TOKENS
            )

            result_text = response.choices[0].message.content.strip()
            result = self._extract_json_from_response(result_text)

            news_items = []
            for idx, news in enumerate(result.get('news', [])):
                # å°è¯•ä»æè¿°é“¾æ¥ä¸­åŒ¹é…ç›¸å…³é“¾æ¥
                source_links = self._match_links_for_news(news, desc_links)

                news_items.append({
                    'title': news.get('title', ''),
                    'content': news.get('content', ''),
                    'entities': news.get('entities', []),
                    'category': news.get('category', 'å…¶ä»–'),
                    'sources': source_links,
                    'index': idx + 1
                })

            self.logger.info(f"Extracted {len(news_items)} news items from comments only")
            return news_items

        except Exception as e:
            self.logger.error(f"Failed to extract news from comments only: {e}")
            # å¦‚æœAIæå–å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []

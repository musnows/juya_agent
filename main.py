#!/usr/bin/env python3
"""
Juya AIæ—©æŠ¥ç”Ÿæˆå™¨
ç›´æ¥å®ç°ä¸šåŠ¡é€»è¾‘ï¼Œæ— éœ€Agentï¼Œæ”¯æŒä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š
1. å•æ¬¡è¿è¡Œï¼šæ‹‰å–æœ€æ–°æ—©æŠ¥è§†é¢‘å¹¶ç”ŸæˆæŠ¥å‘Š
2. æŒ‡å®šBVå·ï¼šå¤„ç†æŒ‡å®šçš„BVå·è§†é¢‘
3. å®šæ—¶è¿è¡Œï¼šæ¯10åˆ†é’Ÿæ£€æµ‹å½“æ—¥AIæ—©æŠ¥
"""
import argparse
import asyncio
import json
import os
import re
import sys
import time
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Optional

from dotenv import load_dotenv

from utils.modules.bilibili_api import BilibiliAPI, parse_cookie_string
from utils.modules.subtitle_processor_ai import AISubtitleProcessor
from utils.modules.email_sender import EmailSender
from utils.web_generator import WebGenerator
from utils.logger import get_logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…¨å±€é…ç½®
PROJECT_ROOT = Path(__file__).resolve().parent
PROCESSED_VIDEOS_PATH = PROJECT_ROOT / "data" / "processed_videos.json"
DOCS_DIR = PROJECT_ROOT / "docs"
COOKIE_FILE = PROJECT_ROOT / "config" / "cookies.json"
DIST_DIR = PROJECT_ROOT / "dist"

# æ©˜é¸¦UPä¸»UID
JUYA_UID = 285286947

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DOCS_DIR.mkdir(exist_ok=True)
(PROJECT_ROOT / "data").mkdir(exist_ok=True)

# åˆ›å»ºå…¨å±€æ—¥å¿—å™¨
logger = get_logger("juya_main")


class JuyaProcessor:
    """æ©˜é¸¦AIæ—©æŠ¥å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        # åˆå§‹åŒ–æ—¥å¿—å™¨
        self.logger = get_logger("juya_processor")

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.api = self._get_bili_api()
        self.processor = AISubtitleProcessor()
        self.email_sender = EmailSender()
    
    def _get_bili_api(self) -> BilibiliAPI:
        """è·å–Bç«™APIå®¢æˆ·ç«¯"""
        if not COOKIE_FILE.exists():
            raise FileNotFoundError(f"è¯·é…ç½® {COOKIE_FILE} æ–‡ä»¶")
        
        with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
            cookies = json.load(f)
        
        return BilibiliAPI(cookies)
    
    def _load_processed_videos(self) -> Dict:
        """åŠ è½½å·²å¤„ç†çš„è§†é¢‘è®°å½•"""
        if PROCESSED_VIDEOS_PATH.exists():
            with open(PROCESSED_VIDEOS_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_processed_videos(self, processed: Dict):
        """ä¿å­˜å·²å¤„ç†çš„è§†é¢‘è®°å½•"""
        with open(PROCESSED_VIDEOS_PATH, 'w', encoding='utf-8') as f:
            json.dump(processed, f, ensure_ascii=False, indent=2)
    
    def _is_ai_early_report(self, video_info: Dict, target_date: date = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAIæ—©æŠ¥è§†é¢‘"""
        title = video_info.get('title', '').lower()
        desc = video_info.get('desc', '').lower()

        # æ£€æŸ¥æ ‡é¢˜å’Œæè¿°ä¸­æ˜¯å¦åŒ…å«AIæ—©æŠ¥ç›¸å…³å…³é”®è¯
        ai_keywords = ['ai', 'äººå·¥æ™ºèƒ½', 'æ—©æŠ¥', 'èµ„è®¯', 'ç§‘æŠ€', 'æŠ€æœ¯']

        # æ ‡é¢˜æ£€æŸ¥
        title_has_ai = any(keyword in title for keyword in ai_keywords)

        # æè¿°æ£€æŸ¥
        desc_has_ai = any(keyword in desc for keyword in ai_keywords)

        # æ£€æŸ¥è§†é¢‘æ—¥æœŸ
        timestamp = video_info.get('pubdate') or video_info.get('created') or 0
        video_date = datetime.fromtimestamp(timestamp)

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼›å¦åˆ™æ£€æŸ¥æ˜¯å¦ä¸ºå½“æ—¥è§†é¢‘
        if target_date:
            is_target_date = video_date.date() == target_date
            date_str = target_date.strftime('%Y-%m-%d')
        else:
            is_target_date = video_date.date() == date.today()
            date_str = "ä»Šæ—¥"

        # è°ƒè¯•ä¿¡æ¯
        if title_has_ai or desc_has_ai:
            self.logger.info(f"ğŸ” æ£€æŸ¥è§†é¢‘: {title[:50]}...")
            self.logger.info(f"   AIå…³é”®è¯: {title_has_ai or desc_has_ai}")
            self.logger.info(f"   æ˜¯å¦{date_str}: {is_target_date}")
            self.logger.info(f"   è§†é¢‘æ—¥æœŸ: {video_date.date()}, {date_str}: {date.today() if not target_date else target_date}")

        return (title_has_ai or desc_has_ai) and is_target_date
    
    def _check_today_report_exists(self) -> bool:
        """æ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²å­˜åœ¨AIæ—©æŠ¥æ–‡ä»¶"""
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        # æœç´¢docsç›®å½•ä¸‹åŒ…å«ä»Šæ—¥æ—¥æœŸçš„mdæ–‡ä»¶
        for md_file in DOCS_DIR.glob(f"*{today_str}*.md"):
            if md_file.is_file():
                self.logger.info(f"âœ… å‘ç°ä»Šæ—¥æ—©æŠ¥æ–‡ä»¶: {md_file.name}")
                return True
        
        return False
    
    def get_latest_ai_report(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„AIæ—©æŠ¥è§†é¢‘BVå·"""
        self.logger.info("ğŸ” æ­£åœ¨æœç´¢æœ€æ–°çš„AIæ—©æŠ¥è§†é¢‘...")

        # è·å–æœ€è¿‘20ä¸ªè§†é¢‘
        videos = self.api.get_user_videos(uid=JUYA_UID, page_size=20)

        for video in videos:
            if self._is_ai_early_report(video):
                bvid = video['bvid']
                title = video['title']
                self.logger.info(f"âœ… æ‰¾åˆ°AIæ—©æŠ¥è§†é¢‘: {title} ({bvid})")
                return bvid

        self.logger.warning("âŒ æœªæ‰¾åˆ°ä»Šæ—¥çš„AIæ—©æŠ¥è§†é¢‘")
        return None

    def get_ai_reports_by_date_range(self, start_date: date, end_date: date) -> List[Dict]:
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰AIæ—©æŠ¥è§†é¢‘"""
        self.logger.info(f"ğŸ” æ­£åœ¨æœç´¢ {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')} çš„AIæ—©æŠ¥è§†é¢‘...")

        ai_reports = []

        # è®¡ç®—éœ€è¦è·å–çš„å¤©æ•°
        days_count = (end_date - start_date).days + 1

        # ç”±äºBç«™APIé™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦è·å–æ›´å¤šè§†é¢‘æ¥è¦†ç›–æ•´ä¸ªæ—¥æœŸèŒƒå›´
        # é€šå¸¸ä¸€å¤©å¯èƒ½æœ‰å¤šä¸ªè§†é¢‘ï¼Œæ‰€ä»¥æˆ‘ä»¬è·å–æ›´å¤šçš„è§†é¢‘
        estimated_videos_needed = days_count * 10  # ä¼°ç®—æ¯å¤©æœ€å¤š10ä¸ªè§†é¢‘
        page_size = min(estimated_videos_needed, 50)  # Bç«™APIé™åˆ¶æœ€å¤š50ä¸ª

        self.logger.info(f"ğŸ“¥ è·å–æœ€è¿‘ {page_size} ä¸ªè§†é¢‘...")

        # è·å–æ›´å¤šè§†é¢‘æ¥è¦†ç›–å†å²æ—¥æœŸèŒƒå›´
        videos = self.api.get_user_videos(uid=JUYA_UID, page_size=page_size)

        # æŒ‰æ—¥æœŸæ£€æŸ¥æ¯ä¸ªè§†é¢‘
        current_date = start_date
        while current_date <= end_date:
            daily_videos = []

            for video in videos:
                if self._is_ai_early_report(video, current_date):
                    daily_videos.append({
                        'bvid': video['bvid'],
                        'title': video['title'],
                        'date': current_date.strftime('%Y-%m-%d'),
                        'pubdate': video.get('pubdate', 0)
                    })

            if daily_videos:
                # é€‰æ‹©å½“å¤©æœ€æ–°çš„è§†é¢‘ï¼ˆé€šå¸¸æ˜¯å‘å¸ƒæ—¶é—´æœ€æ™šçš„ï¼‰
                latest_video = max(daily_videos, key=lambda x: x['pubdate'])
                ai_reports.append(latest_video)
                self.logger.info(f"âœ… {current_date.strftime('%Y-%m-%d')}: æ‰¾åˆ°AIæ—©æŠ¥ {latest_video['title']}")
            else:
                self.logger.warning(f"âš ï¸ {current_date.strftime('%Y-%m-%d')}: æœªæ‰¾åˆ°AIæ—©æŠ¥")

            current_date += timedelta(days=1)

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        ai_reports.sort(key=lambda x: x['date'], reverse=True)

        self.logger.info(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(ai_reports)} ä¸ªAIæ—©æŠ¥è§†é¢‘")
        return ai_reports
    
    def process_video(self, bvid: str, force_regenerate: bool = False) -> bool:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        self.logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘: {bvid}")

        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.api.get_video_info(bvid)
            video_date = datetime.fromtimestamp(video_info['pubdate'])
            date_str = video_date.strftime('%Y-%m-%d')
            filename = f"{date_str}_AIæ—©æŠ¥_{bvid}.md"
            filepath = DOCS_DIR / filename

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not force_regenerate and filepath.exists():
                self.logger.info(f"ğŸ“„ æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡é‡æ–°ç”Ÿæˆ: {filepath}")
                return True

            # è·å–å­—å¹•
            self.logger.info("ğŸ“¥ è·å–å­—å¹•...")
            subtitle = self.api.get_subtitle(bvid)

            if not subtitle:
                self.logger.warning("âš ï¸ è§†é¢‘æ²¡æœ‰å­—å¹•ï¼Œå°†ä½¿ç”¨è§†é¢‘ç®€ä»‹æå–æ–°é—»...")

            # å¤„ç†å­—å¹•/ç®€ä»‹
            self.logger.info("ğŸ¤– AIæ•´ç†æ—©æŠ¥ä¸­...")
            processed_data = self.processor.process(
                subtitle if subtitle else [],
                video_info
            )

            # ç”ŸæˆMarkdownæ–‡æ¡£
            self.logger.info("ğŸ“ ç”Ÿæˆæ–‡æ¡£...")
            markdown = self.processor.format_markdown(processed_data)

            # ä¿å­˜Markdownæ–‡æ¡£
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown)

            self.logger.info(f"âœ… æ–‡æ¡£å·²ç”Ÿæˆ: {filepath}")

            # ç”ŸæˆJSONæ–‡ä»¶
            json_filepath = self._generate_json_file(processed_data, video_info, bvid, filepath)
            self.logger.info(f"âœ… JSONæ–‡ä»¶å·²ç”Ÿæˆ: {json_filepath}")

            # æ›´æ–°å¤„ç†è®°å½•
            processed = self._load_processed_videos()
            processed[bvid] = {
                'title': video_info['title'],
                'processed_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'subtitle_path': str(filepath),
                'json_path': str(json_filepath)
            }
            self._save_processed_videos(processed)

            return True

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†è§†é¢‘å¤±è´¥: {e}")
            return False
    
    def _generate_json_file(self, processed_data: Dict, video_info: Dict, bvid: str, md_filepath: str) -> str:
        """ç”ŸæˆJSONæ–‡ä»¶"""
        try:
            # æå–æ–°é—»æ•°æ®
            news_items = processed_data.get('news_items', [])
            overview = processed_data.get('overview', {})
            
            # æ„å»ºdataæ•°ç»„
            data_array = []
            for index,item in enumerate(news_items, 1):
                data_array.append({
                    "index": index,
                    "title": item.get('title', ''),
                    "content": item.get('content', ''),
                    "sources": item.get('sources',[])
                })
            
            # æ„å»ºå®Œæ•´çš„JSONç»“æ„
            json_data = {
                "data": data_array,
                "created_time": overview.get('processed_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                "title": video_info.get('title', ''),
                "date": overview.get('publish_date', datetime.fromtimestamp(video_info.get('pubdate', 0)).strftime('%Y-%m-%d'))
            }
            
            # ç”ŸæˆJSONæ–‡ä»¶è·¯å¾„ï¼ˆä¸MDæ–‡ä»¶åŒç›®å½•åŒåï¼‰
            md_path = Path(md_filepath)
            json_filepath = md_path.with_suffix('.json')
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            return str(json_filepath)
            
        except Exception as e:
            self.logger.error(f"âŒ ç”ŸæˆJSONæ–‡ä»¶å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è·¯å¾„
            md_path = Path(md_filepath)
            return str(md_path.with_suffix('.json'))
    
    def send_email_report(self, bvid: str, to_email: str = None) -> bool:
        """å‘é€é‚®ä»¶æŠ¥å‘Š"""
        try:
            to_email = to_email or os.getenv('EMAIL_TO')
            if not to_email:
                self.logger.error("âŒ æœªé…ç½®æ”¶ä»¶äººé‚®ç®±")
                return False

            # æ£€æŸ¥å¤„ç†è®°å½•
            processed = self._load_processed_videos()
            if bvid not in processed:
                self.logger.error(f"âŒ è§†é¢‘ {bvid} å°šæœªå¤„ç†")
                return False

            md_path = processed[bvid].get('subtitle_path')
            if not md_path or not os.path.exists(md_path):
                self.logger.error(f"âŒ æœªæ‰¾åˆ°å¤„ç†æ–‡æ¡£: {md_path}")
                return False
            
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.api.get_video_info(bvid)
            
            # è§£æMarkdownæ–‡ä»¶ç”ŸæˆHTMLé‚®ä»¶
            html_content = self._generate_email_html(md_path)
            
            # å‘é€é‚®ä»¶
            success = self.email_sender.send_video_report(
                to_email=to_email,
                video_title=video_info['title'],
                bvid=bvid,
                html_content=html_content,
                markdown_path=md_path
            )
            
            if success:
                self.logger.info(f"âœ… é‚®ä»¶å·²å‘é€åˆ° {to_email}")
            else:
                self.logger.error("âŒ é‚®ä»¶å‘é€å¤±è´¥")

            return success

        except Exception as e:
            self.logger.error(f"âŒ å‘é€é‚®ä»¶å¤±è´¥: {e}")
            return False
    
    def _generate_email_html(self, md_path: str) -> str:
        """ä»Markdownæ–‡ä»¶ç”ŸæˆHTMLé‚®ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>æ©˜é¸¦AIæ—©æŠ¥</title>
</head>
<body style="font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px;">
    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 8px;">
        <pre style="white-space: pre-wrap; font-family: inherit;">{content}</pre>
    </div>
</body>
</html>
"""
        return html

    def process_history_reports(self, days: int = 30, force_regenerate: bool = False) -> Dict:
        """å¤„ç†å†å²AIæ—©æŠ¥"""
        self.logger.info(f"ğŸ“š å¼€å§‹å¤„ç†å†å² {days} å¤©çš„AIæ—©æŠ¥...")

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = date.today() - timedelta(days=1)  # ä¸åŒ…æ‹¬ä»Šå¤©
        start_date = end_date - timedelta(days=days - 1)

        self.logger.info(f"ğŸ“… å¤„ç†æ—¥æœŸèŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}")

        # è·å–æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰AIæ—©æŠ¥è§†é¢‘
        ai_reports = self.get_ai_reports_by_date_range(start_date, end_date)

        if not ai_reports:
            self.logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•å†å²AIæ—©æŠ¥è§†é¢‘")
            return {
                'total_found': 0,
                'total_processed': 0,
                'total_skipped': 0,
                'total_failed': 0,
                'reports': []
            }

        # å¤„ç†æ¯ä¸ªè§†é¢‘
        results = []
        processed_count = 0
        skipped_count = 0
        failed_count = 0

        for report in ai_reports:
            bvid = report['bvid']
            title = report['title']
            report_date = report['date']

            self.logger.info(f"\nğŸ¬ å¤„ç† {report_date} çš„è§†é¢‘: {title}")
            self.logger.info(f"   BVå·: {bvid}")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡æ¡£
            video_info = self.api.get_video_info(bvid)
            video_date = datetime.fromtimestamp(video_info['pubdate'])
            date_str = video_date.strftime('%Y-%m-%d')
            filename = f"{date_str}_AIæ—©æŠ¥_{bvid}.md"
            filepath = DOCS_DIR / filename

            if not force_regenerate and filepath.exists():
                self.logger.info(f"   â­ï¸  æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
                skipped_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'skipped',
                    'reason': 'å·²å­˜åœ¨'
                })
                continue

            # å¤„ç†è§†é¢‘
            success = self.process_video(bvid, force_regenerate=force_regenerate)

            if success:
                processed_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'success',
                    'reason': 'å¤„ç†æˆåŠŸ'
                })
                self.logger.info(f"   âœ… å¤„ç†æˆåŠŸ")
            else:
                failed_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'failed',
                    'reason': 'å¤„ç†å¤±è´¥'
                })
                self.logger.error(f"   âŒ å¤„ç†å¤±è´¥")

        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.logger.info(f"\nğŸ“Š å†å²å¤„ç†å®Œæˆç»Ÿè®¡:")
        self.logger.info(f"   æ‰¾åˆ°è§†é¢‘: {len(ai_reports)} ä¸ª")
        self.logger.info(f"   æˆåŠŸå¤„ç†: {processed_count} ä¸ª")
        self.logger.info(f"   è·³è¿‡å·²å­˜åœ¨: {skipped_count} ä¸ª")
        self.logger.info(f"   å¤„ç†å¤±è´¥: {failed_count} ä¸ª")

        return {
            'total_found': len(ai_reports),
            'total_processed': processed_count,
            'total_skipped': skipped_count,
            'total_failed': failed_count,
            'date_range': {
                'start': start_date.strftime('%Y-%m-%d'),
                'end': end_date.strftime('%Y-%m-%d')
            },
            'reports': results
        }


def single_run(processor: JuyaProcessor, send_email: bool = False, generate_web: bool = False):
    """å•æ¬¡è¿è¡Œæ¨¡å¼ï¼šè·å–æœ€æ–°AIæ—©æŠ¥"""
    logger.info("="*60)
    logger.info("ğŸš€ å•æ¬¡è¿è¡Œæ¨¡å¼ - è·å–æœ€æ–°AIæ—©æŠ¥")
    logger.info("="*60)

    # è·å–æœ€æ–°AIæ—©æŠ¥è§†é¢‘
    bvid = processor.get_latest_ai_report()
    if not bvid:
        logger.error("âŒ æœªæ‰¾åˆ°AIæ—©æŠ¥è§†é¢‘")
        return False

    # å¤„ç†è§†é¢‘
    success = processor.process_video(bvid)
    if not success:
        logger.error("âŒ å¤„ç†è§†é¢‘å¤±è´¥")
        return False

    # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if send_email:
        processor.send_email_report(bvid)

    # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if generate_web:
        logger.info("\nğŸŒ ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™...")
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
        web_result = web_generator.generate_static_site()
        if web_result:
            logger.info("âœ… é™æ€å‰ç«¯ç½‘ç«™å·²æ›´æ–°")
        else:
            logger.error("âŒ é™æ€å‰ç«¯ç½‘ç«™ç”Ÿæˆå¤±è´¥")

    logger.info("âœ… å•æ¬¡è¿è¡Œå®Œæˆ")
    return True


def bv_run(processor: JuyaProcessor, bvid: str, send_email: bool = False, generate_web: bool = False):
    """æŒ‡å®šBVå·è¿è¡Œæ¨¡å¼"""
    logger.info("="*60)
    logger.info(f"ğŸ¯ æŒ‡å®šBVå·è¿è¡Œæ¨¡å¼ - {bvid}")
    logger.info("="*60)

    # å¤„ç†è§†é¢‘
    success = processor.process_video(bvid)
    if not success:
        logger.error("âŒ å¤„ç†è§†é¢‘å¤±è´¥")
        return False

    # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if send_email:
        processor.send_email_report(bvid)

    # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if generate_web:
        logger.info("\nğŸŒ ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™...")
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
        web_result = web_generator.generate_static_site()
        if web_result:
            logger.info("âœ… é™æ€å‰ç«¯ç½‘ç«™å·²æ›´æ–°")
        else:
            logger.error("âŒ é™æ€å‰ç«¯ç½‘ç«™ç”Ÿæˆå¤±è´¥")

    logger.info("âœ… BVå·è¿è¡Œå®Œæˆ")
    return True


def loop_run(processor: JuyaProcessor, send_email: bool = False, generate_web: bool = False):
    """å®šæ—¶è¿è¡Œæ¨¡å¼ï¼šæ¯15åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡ï¼Œ0-7ç‚¹è·³è¿‡æ£€æŸ¥"""
    logger.info("="*60)
    logger.info("â° å®šæ—¶è¿è¡Œæ¨¡å¼ - æ¯15åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡ï¼Œ0-7ç‚¹è·³è¿‡æ£€æŸ¥")
    logger.info("="*60)

    if generate_web:
        logger.info("ğŸŒ å¯ç”¨è‡ªåŠ¨å‰ç«¯æ›´æ–°æ¨¡å¼")

    check_interval = 900  # 15åˆ†é’Ÿ

    try:
        while True:
            current_time = datetime.now()
            current_hour = current_time.hour

            # æ£€æŸ¥æ˜¯å¦åœ¨è·³è¿‡æ—¶é—´ï¼ˆ0-7ç‚¹ï¼‰
            if 0 <= current_hour < 7:
                logger.info(f"\nğŸŒ™ å½“å‰æ—¶é—´ {current_time.strftime('%Y-%m-%d %H:%M:%S')} å¤„äºè·³è¿‡æ—¶æ®µï¼ˆ0-7ç‚¹ï¼‰ï¼Œä¸è¿›è¡Œæ£€æµ‹")
                logger.info(f"ğŸ’¤ ç­‰å¾… {check_interval // 60} åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æ£€æµ‹...")
                time.sleep(check_interval)
                continue

            logger.info(f"\nğŸ• {current_time.strftime('%Y-%m-%d %H:%M:%S')} - å¼€å§‹æ£€æµ‹...")

            # æ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰æŠ¥å‘Š
            if processor._check_today_report_exists():
                logger.info("ğŸ“„ ä»Šæ—¥AIæ—©æŠ¥å·²å­˜åœ¨ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æµ‹")
            else:
                # è·å–æœ€æ–°AIæ—©æŠ¥
                bvid = processor.get_latest_ai_report()
                if bvid:
                    # å¤„ç†è§†é¢‘
                    success = processor.process_video(bvid)
                    if success:
                        # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if send_email:
                            processor.send_email_report(bvid)

                        # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if generate_web:
                            logger.info("ğŸŒ æ›´æ–°é™æ€å‰ç«¯ç½‘ç«™...")
                            web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
                            web_result = web_generator.generate_static_site()
                            if web_result:
                                logger.info("âœ… é™æ€å‰ç«¯ç½‘ç«™å·²æ›´æ–°")
                            else:
                                logger.error("âŒ é™æ€å‰ç«¯ç½‘ç«™ç”Ÿæˆå¤±è´¥")
                else:
                    logger.info("ğŸ“­ æš‚æ— æ–°çš„AIæ—©æŠ¥")

            logger.info(f"ğŸ’¤ ç­‰å¾… {check_interval // 60} åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æ£€æµ‹...")
            time.sleep(check_interval)

    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ å®šæ—¶è¿è¡Œå·²åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ å®šæ—¶è¿è¡Œå‡ºé”™: {e}")


def history_run(processor: JuyaProcessor, days: int = 30, force: bool = False, generate_web: bool = False):
    """å†å²è¿è¡Œæ¨¡å¼ï¼šå¤„ç†æŒ‡å®šå¤©æ•°çš„å†å²AIæ—©æŠ¥"""
    logger.info("="*60)
    logger.info(f"ğŸ“š å†å²è¿è¡Œæ¨¡å¼ - å¤„ç†æœ€è¿‘ {days} å¤©çš„AIæ—©æŠ¥")
    logger.info("="*60)

    # å¤„ç†å†å²æŠ¥å‘Š
    result = processor.process_history_reports(days=days, force_regenerate=force)

    # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
    if result['total_found'] > 0:
        logger.info(f"\nğŸ‰ å†å²å¤„ç†å®Œæˆï¼")
        logger.info(f"ğŸ“‹ å¤„ç†æ‘˜è¦:")
        logger.info(f"   å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"   æ—¥æœŸèŒƒå›´: {result['date_range']['start']} åˆ° {result['date_range']['end']}")
        logger.info(f"   æ‰¾åˆ°è§†é¢‘: {result['total_found']} ä¸ª")
        logger.info(f"   æˆåŠŸå¤„ç†: {result['total_processed']} ä¸ª")
        logger.info(f"   è·³è¿‡å·²å­˜åœ¨: {result['total_skipped']} ä¸ª")
        logger.info(f"   å¤„ç†å¤±è´¥: {result['total_failed']} ä¸ª")

        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_filename = f"history_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path = DOCS_DIR / report_filename

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if generate_web:
            logger.info("\nğŸŒ ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™...")
            web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
            web_result = web_generator.generate_static_site()
            if web_result:
                logger.info("âœ… é™æ€å‰ç«¯ç½‘ç«™å·²æ›´æ–°")
            else:
                logger.error("âŒ é™æ€å‰ç«¯ç½‘ç«™ç”Ÿæˆå¤±è´¥")
    else:
        logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•å†å²AIæ—©æŠ¥è§†é¢‘")

    return result


def web_run(processor: JuyaProcessor):
    """Webè¿è¡Œæ¨¡å¼ï¼šç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™"""
    logger.info("="*60)
    logger.info("ğŸŒ Webè¿è¡Œæ¨¡å¼ - ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™")
    logger.info("="*60)

    try:
        # åˆ›å»ºWebç”Ÿæˆå™¨
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)

        logger.info("ğŸ“ å‡†å¤‡ç”Ÿæˆé™æ€å‰ç«¯...")
        logger.info(f"   æºç›®å½•: {DOCS_DIR}")
        logger.info(f"   è¾“å‡ºç›®å½•: {DIST_DIR}")

        # ç”Ÿæˆé™æ€ç½‘ç«™
        result = web_generator.generate_static_site()

        if result:
            logger.info("âœ… é™æ€å‰ç«¯ç½‘ç«™ç”ŸæˆæˆåŠŸï¼")
            logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {DIST_DIR}")
            logger.info(f"ğŸ“„ ä¸»é¡µé¢: {DIST_DIR}/index.html")
            logger.info("\nğŸš€ è¦æŸ¥çœ‹ç½‘ç«™ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€:")
            logger.info(f"   file://{DIST_DIR}/index.html")
        else:
            logger.error("âŒ é™æ€å‰ç«¯ç½‘ç«™ç”Ÿæˆå¤±è´¥")
            return False

        return True

    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆé™æ€å‰ç«¯å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Juya AIæ—©æŠ¥ç”Ÿæˆå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¿è¡Œç¤ºä¾‹:
  %(prog)s                           # å•æ¬¡è¿è¡Œï¼Œè·å–æœ€æ–°AIæ—©æŠ¥
  %(prog)s --single                  # åŒä¸Šï¼Œæ˜¾å¼æŒ‡å®šå•æ¬¡è¿è¡Œ
  %(prog)s --bv BV1234567890        # å¤„ç†æŒ‡å®šBVå·è§†é¢‘
  %(prog)s --loop                    # å®šæ—¶è¿è¡Œï¼Œæ¯15åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡ï¼Œ0-7ç‚¹è·³è¿‡æ£€æŸ¥
  %(prog)s --history                 # å¤„ç†å†å²30å¤©çš„AIæ—©æŠ¥
  %(prog)s --history 15              # å¤„ç†å†å²15å¤©çš„AIæ—©æŠ¥
  %(prog)s --history 30 --force      # å¼ºåˆ¶é‡æ–°ç”Ÿæˆå†å²30å¤©çš„AIæ—©æŠ¥
  %(prog)s --web                     # ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™åˆ°distç›®å½•

ç»„åˆé€‰é¡¹:
  %(prog)s --web                     # ä»…ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™
  %(prog)s --single --web            # å•æ¬¡è¿è¡Œå¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --bv BV1234567890 --web   # å¤„ç†æŒ‡å®šBVå·å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --loop --web              # å®šæ—¶è¿è¡Œå¹¶è‡ªåŠ¨æ›´æ–°é™æ€å‰ç«¯
  %(prog)s --history --web           # å¤„ç†å†å²æ—©æŠ¥å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --send-email --web        # å‘é€é‚®ä»¶å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --history --force --web   # å¼ºåˆ¶é‡æ–°ç”Ÿæˆå†å²æ—©æŠ¥å¹¶æ›´æ–°é™æ€å‰ç«¯
        """
    )

    # è¿è¡Œæ¨¡å¼å‚æ•°ï¼ˆäº’æ–¥ï¼‰
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--single', action='store_true', help='å•æ¬¡è¿è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰')
    mode_group.add_argument('--bv', type=str, help='æŒ‡å®šBVå·è¿è¡Œæ¨¡å¼')
    mode_group.add_argument('--loop', action='store_true', help='å®šæ—¶è¿è¡Œæ¨¡å¼')
    mode_group.add_argument('--history', nargs='?', type=int, const=30, metavar='DAYS', help='å¤„ç†å†å²æŒ‡å®šå¤©æ•°çš„AIæ—©æŠ¥ï¼ˆé»˜è®¤30å¤©ï¼‰')

    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ–‡æ¡£')
    parser.add_argument('--send-email', action='store_true', help='å¤„ç†å®Œæˆåå‘é€é‚®ä»¶')
    parser.add_argument('--web', action='store_true', help='å¤„ç†å®Œæˆåç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™ï¼ˆå¯ä¸å…¶ä»–å‚æ•°ç»„åˆä½¿ç”¨ï¼‰')

    args = parser.parse_args()

    # ç¡®å®šè¿è¡Œæ¨¡å¼
    if args.loop:
        mode = 'loop'
    elif args.bv:
        mode = 'bv'
    elif args.history is not None:
        mode = 'history'
    else:
        mode = 'single'  # é»˜è®¤æ¨¡å¼

    # åˆå§‹åŒ–å¤„ç†å™¨
    try:
        processor = JuyaProcessor()
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

    # æ‰§è¡Œå¯¹åº”çš„è¿è¡Œæ¨¡å¼
    try:
        # å¦‚æœåªæœ‰--webå‚æ•°ï¼ˆæ²¡æœ‰å…¶ä»–ä»»ä½•å‚æ•°ï¼‰ï¼Œæ‰§è¡Œçº¯webç”Ÿæˆ
        if args.web and not args.single and not args.bv and not args.loop and args.history is None and not args.send_email and not args.force:
            web_run(processor)
        elif mode == 'single':
            single_run(processor, args.send_email, args.web)
        elif mode == 'bv':
            bv_run(processor, args.bv, args.send_email, args.web)
        elif mode == 'loop':
            loop_run(processor, args.send_email, args.web)
        elif mode == 'history':
            history_run(processor, days=args.history, force=args.force, generate_web=args.web)
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ ç¨‹åºå·²åœæ­¢")
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
Juya AIæ—©æŠ¥ç”Ÿæˆå™¨
ç›´æ¥å®ç°ä¸šåŠ¡é€»è¾‘ï¼Œæ— éœ€Agentï¼Œæ”¯æŒä¸‰ç§è¿è¡Œæ¨¡å¼ï¼š
1. å•æ¬¡è¿è¡Œï¼šæ‹‰å–æœ€æ–°æ—©æŠ¥è§†é¢‘å¹¶ç”ŸæˆæŠ¥å‘Š
2. æŒ‡å®šBVå·ï¼šå¤„ç†æŒ‡å®šçš„BVå·è§†é¢‘
3. å®šæ—¶è¿è¡Œï¼šæ¯10åˆ†é’Ÿæ£€æµ‹å½“æ—¥AIæ—©æŠ¥
"""
import argparse
import asyncio
import json
import os
import re
import sys
import time
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import Dict, List, Optional

from dotenv import load_dotenv

from utils.modules.bilibili_api import BilibiliAPI
from utils.modules.subtitle_processor_ai import AISubtitleProcessor
from utils.modules.email_sender import EmailSender
from utils.video_fallback import VideoFallbackProcessor
from utils.web_generator import WebGenerator
from utils.logger import get_logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…¨å±€é…ç½®
PROJECT_ROOT = Path(__file__).resolve().parent
PROCESSED_VIDEOS_PATH = PROJECT_ROOT / "data" / "processed_videos.json"
DOCS_DIR = PROJECT_ROOT / "docs"
COOKIE_FILE = PROJECT_ROOT / "config" / "cookies.json"
DIST_DIR = PROJECT_ROOT / "dist"

# æ©˜é¸¦UPä¸»UID
JUYA_UID = 285286947

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DOCS_DIR.mkdir(exist_ok=True)
(PROJECT_ROOT / "data").mkdir(exist_ok=True)

# ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—å™¨
logger = get_logger()


class JuyaProcessor:
    """æ©˜é¸¦AIæ—©æŠ¥å¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—å™¨
        self.logger = get_logger()

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.api = self._get_bili_api()
        self.email_sender = EmailSender()
        self.fallback_processor = VideoFallbackProcessor(PROJECT_ROOT)

        # åˆå§‹åŒ–å¤„ç†å™¨ï¼Œä¼ å…¥è§†é¢‘æ•°æ®ç›®å½•
        self.processor = AISubtitleProcessor(self.fallback_processor.video_dir)
    
    def _get_bili_api(self) -> BilibiliAPI:
        """è·å–Bç«™APIå®¢æˆ·ç«¯"""
        if not COOKIE_FILE.exists():
            raise FileNotFoundError(f"è¯·é…ç½® {COOKIE_FILE} æ–‡ä»¶")
        
        with open(COOKIE_FILE, 'r', encoding='utf-8') as f:
            cookies = json.load(f)
        
        return BilibiliAPI(cookies)
    
    def _load_processed_videos(self) -> Dict:
        """åŠ è½½å·²å¤„ç†çš„è§†é¢‘è®°å½•"""
        if PROCESSED_VIDEOS_PATH.exists():
            with open(PROCESSED_VIDEOS_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_processed_videos(self, processed: Dict):
        """ä¿å­˜å·²å¤„ç†çš„è§†é¢‘è®°å½•"""
        with open(PROCESSED_VIDEOS_PATH, 'w', encoding='utf-8') as f:
            json.dump(processed, f, ensure_ascii=False, indent=2)
    
    def _is_ai_early_report(self, video_info: Dict, target_date: date = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAIæ—©æŠ¥è§†é¢‘"""
        title = video_info.get('title', '')
        desc = video_info.get('desc', '')

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æµ‹"AIæ—©æŠ¥"ï¼Œå¤„ç†ä»»æ„æ•°é‡çš„ç©ºæ ¼å’Œå¤§å°å†™
        # ai\s*æ—©æŠ¥ åŒ¹é… "AI" + ä»»æ„æ•°é‡ç©ºæ ¼ + "æ—©æŠ¥"
        ai_early_report_pattern = re.compile(r'AI\s*æ—©æŠ¥', re.IGNORECASE)

        # æ£€æŸ¥æ ‡é¢˜ä¸­çš„AIæ—©æŠ¥å…³é”®è¯
        title_has_ai_early_report = bool(ai_early_report_pattern.search(title))

        # å¦‚æœæ ‡é¢˜ä¸­æ²¡æœ‰æ‰¾åˆ°"AIæ—©æŠ¥"ï¼Œåˆ™æ£€æŸ¥æ˜¯å¦åªæœ‰"æ—©æŠ¥"å…³é”®è¯ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        if not title_has_ai_early_report:
            # åªåŒ¹é…"æ—©æŠ¥"å…³é”®è¯
            early_report_pattern = re.compile(r'æ—©æŠ¥')
            title_has_early_report = bool(early_report_pattern.search(title))
        else:
            title_has_early_report = True  # å¦‚æœæ‰¾åˆ°AIæ—©æŠ¥ï¼Œåˆ™è®¤ä¸ºæ»¡è¶³æ—©æŠ¥æ¡ä»¶

        # æ£€æŸ¥æè¿°ä¸­çš„å…³é”®è¯ï¼ˆä½œä¸ºè¾…åŠ©åˆ¤æ–­ï¼Œä¸»è¦ç”¨äºæ²¡æœ‰æ˜ç¡®æ ‡é¢˜çš„æƒ…å†µï¼‰
        desc_has_ai_early_report = bool(ai_early_report_pattern.search(desc))

        # æ£€æŸ¥è§†é¢‘æ—¥æœŸ
        timestamp = video_info.get('pubdate') or video_info.get('created') or 0
        video_date = datetime.fromtimestamp(timestamp)

        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡æ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼›å¦åˆ™æ£€æŸ¥æ˜¯å¦ä¸ºå½“æ—¥è§†é¢‘
        if target_date:
            is_target_date = video_date.date() == target_date
            date_str = target_date.strftime('%Y-%m-%d')
        else:
            is_target_date = video_date.date() == date.today()
            date_str = "ä»Šæ—¥"

        # åˆ¤æ–­é€»è¾‘ï¼šå¿…é¡»æ»¡è¶³æ—¥æœŸæ¡ä»¶ï¼Œä¸”æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
        # 1. æ ‡é¢˜åŒ…å«"AIæ—©æŠ¥"ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        # 2. æ ‡é¢˜åŒ…å«"æ—©æŠ¥"ï¼ˆå¤‡é€‰æ¡ä»¶ï¼‰
        # 3. æè¿°åŒ…å«"AIæ—©æŠ¥"ï¼ˆè¾…åŠ©æ¡ä»¶ï¼‰
        is_ai_report = (title_has_ai_early_report or title_has_early_report or desc_has_ai_early_report)

        # è°ƒè¯•ä¿¡æ¯
        if is_ai_report:
            self.logger.info(f"Checking video: {title[:50]}..., AIæ—©æŠ¥ keywords: title_ai_early={title_has_ai_early_report}, title_early={title_has_early_report}, desc_ai_early={desc_has_ai_early_report}, is {date_str}: {is_target_date}, video date: {video_date.date()}, target: {date.today() if not target_date else target_date}")

        return is_ai_report and is_target_date
    
    def _check_today_report_exists(self) -> bool:
        """æ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²å­˜åœ¨AIæ—©æŠ¥æ–‡ä»¶"""
        today_str = datetime.now().strftime('%Y-%m-%d')
        
        # æœç´¢docsç›®å½•ä¸‹åŒ…å«ä»Šæ—¥æ—¥æœŸçš„mdæ–‡ä»¶
        for md_file in DOCS_DIR.glob(f"*{today_str}*.md"):
            if md_file.is_file():
                self.logger.info(f"Found today's report file: {md_file.name}")
                return True
        
        return False
    
    def get_latest_ai_report(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„AIæ—©æŠ¥è§†é¢‘BVå·"""
        self.logger.info("Searching for latest AI report video...")

        # è·å–æœ€è¿‘20ä¸ªè§†é¢‘
        videos = self.api.get_user_videos(uid=JUYA_UID, page_size=20)

        for video in videos:
            if self._is_ai_early_report(video):
                bvid = video['bvid']
                title = video['title']
                self.logger.info(f"Found AI report video: {title} ({bvid})")
                return bvid

        self.logger.warning("No AI report video found for today")
        return None

    def get_ai_reports_by_date_range(self, start_date: date, end_date: date) -> List[Dict]:
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰AIæ—©æŠ¥è§†é¢‘"""
        self.logger.info(f"Searching for AI report videos from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")

        ai_reports = []

        # è®¡ç®—éœ€è¦è·å–çš„å¤©æ•°
        days_count = (end_date - start_date).days + 1

        # ç”±äºBç«™APIé™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦è·å–æ›´å¤šè§†é¢‘æ¥è¦†ç›–æ•´ä¸ªæ—¥æœŸèŒƒå›´
        # é€šå¸¸ä¸€å¤©å¯èƒ½æœ‰å¤šä¸ªè§†é¢‘ï¼Œæ‰€ä»¥æˆ‘ä»¬è·å–æ›´å¤šçš„è§†é¢‘
        estimated_videos_needed = days_count * 10  # ä¼°ç®—æ¯å¤©æœ€å¤š10ä¸ªè§†é¢‘
        page_size = min(estimated_videos_needed, 50)  # Bç«™APIé™åˆ¶æœ€å¤š50ä¸ª

        self.logger.info(f"Fetching {page_size} videos...")

        # è·å–æ›´å¤šè§†é¢‘æ¥è¦†ç›–å†å²æ—¥æœŸèŒƒå›´
        videos = self.api.get_user_videos(uid=JUYA_UID, page_size=page_size)

        # æŒ‰æ—¥æœŸæ£€æŸ¥æ¯ä¸ªè§†é¢‘
        current_date = start_date
        while current_date <= end_date:
            daily_videos = []

            for video in videos:
                if self._is_ai_early_report(video, current_date):
                    daily_videos.append({
                        'bvid': video['bvid'],
                        'title': video['title'],
                        'date': current_date.strftime('%Y-%m-%d'),
                        'pubdate': video.get('pubdate', 0)
                    })

            if daily_videos:
                # é€‰æ‹©å½“å¤©æœ€æ–°çš„è§†é¢‘ï¼ˆé€šå¸¸æ˜¯å‘å¸ƒæ—¶é—´æœ€æ™šçš„ï¼‰
                latest_video = max(daily_videos, key=lambda x: x['pubdate'])
                ai_reports.append(latest_video)
                self.logger.info(f"Found AI report for {current_date.strftime('%Y-%m-%d')}: {latest_video['title']}")
            else:
                self.logger.warning(f"No AI report found for {current_date.strftime('%Y-%m-%d')}")

            current_date += timedelta(days=1)

        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
        ai_reports.sort(key=lambda x: x['date'], reverse=True)

        self.logger.info(f"Found {len(ai_reports)} AI report videos in total")
        return ai_reports
    
    def process_video(self, bvid: str, force_regenerate: bool = False) -> bool:
        """å¤„ç†å•ä¸ªè§†é¢‘"""
        self.logger.info(f"Starting video processing: {bvid}")

        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.api.get_video_info(bvid)
            video_date = datetime.fromtimestamp(video_info['pubdate'])
            date_str = video_date.strftime('%Y-%m-%d')
            date_str_yyyymmdd = video_date.strftime('%Y%m%d')  # ç”¨äºè§†é¢‘ä¸‹è½½çš„æ—¥æœŸæ ¼å¼
            filename = f"{date_str}_AIæ—©æŠ¥_{bvid}.md"
            filepath = DOCS_DIR / filename

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not force_regenerate and filepath.exists():
                self.logger.info(f"Document already exists, skipping regeneration: {filepath}")
                return True

            # è¿™éƒ¨åˆ†é€»è¾‘ç°åœ¨ç§»åˆ°åé¢ï¼Œå› ä¸ºéœ€è¦å…ˆæ£€æŸ¥å­—å¹•æƒ…å†µ

            # è·å–å­—å¹•
            self.logger.info("Fetching subtitles...")
            subtitle = self.api.get_subtitle(bvid)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘å…œåº•é€»è¾‘
            speech_texts = None
            should_use_fallback = False
            has_subtitle = bool(subtitle)

            # æ–°çš„å…œåº•é€»è¾‘ï¼šæ— å­—å¹•æ—¶åªè¦è…¾è®¯äº‘SDKå¯ç”¨ï¼Œéƒ½éœ€è¦ç”Ÿæˆè¯­éŸ³è½¬å†™
            if self.fallback_processor.should_trigger_fallback(video_info, has_subtitle):
                self.logger.info("No subtitles available, triggering video fallback processing for speech-to-text")
                speech_texts = self.fallback_processor.process_video_fallback(bvid, video_info, date_str_yyyymmdd)
                should_use_fallback = speech_texts is not None

            # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶ç”Ÿæˆï¼ˆè§†é¢‘å­—å¹•ã€ç®€ä»‹ã€è¯­éŸ³è½¬å†™å‡ä¸å¯ç”¨ï¼‰
            if self.fallback_processor.should_skip_file_generation(video_info, has_subtitle, bool(should_use_fallback)):
                self.logger.warning("Skipping file generation: insufficient content and no speech SDK available")
                return False

            # å¤„ç†å­—å¹•/ç®€ä»‹/è¯­éŸ³è½¬æ–‡å­—
            self.logger.info("Processing AI report generation...")
            processed_data = self.processor.process(
                subtitle if subtitle else [],
                video_info,
                speech_texts if should_use_fallback else None
            )

            # ç”ŸæˆMarkdownæ–‡æ¡£
            self.logger.info("Generating document...")
            markdown = self.processor.format_markdown(processed_data)

            # ä¿å­˜Markdownæ–‡æ¡£
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(markdown)

            self.logger.info(f"Document generated: {filepath}")

            # ç”ŸæˆJSONæ–‡ä»¶
            json_filepath = self._generate_json_file(processed_data, video_info, bvid, filepath, should_use_fallback)
            self.logger.info(f"JSON file generated: {json_filepath}")

            # æ›´æ–°å¤„ç†è®°å½•
            processed = self._load_processed_videos()
            processed[bvid] = {
                'title': video_info['title'],
                'processed_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'subtitle_path': str(filepath),
                'json_path': str(json_filepath)
            }
            self._save_processed_videos(processed)

            return True

        except Exception as e:
            self.logger.error(f"Failed to process video: {e}")
            return False
    
    def _generate_json_file(self, processed_data: Dict, video_info: Dict, bvid: str, md_filepath: str, video_fallback: bool = False) -> str:
        """ç”ŸæˆJSONæ–‡ä»¶"""
        try:
            # æå–æ–°é—»æ•°æ®
            news_items = processed_data.get('news_items', [])
            overview = processed_data.get('overview', {})

            # æ„å»ºdataæ•°ç»„
            data_array = []
            for index,item in enumerate(news_items, 1):
                data_array.append({
                    "index": index,
                    "title": item.get('title', ''),
                    "content": item.get('content', ''),
                    "sources": item.get('sources',[])
                })

            # æ„å»ºå®Œæ•´çš„JSONç»“æ„
            json_data = {
                "data": data_array,
                "created_time": overview.get('processed_time', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                "title": video_info.get('title', ''),
                "date": overview.get('publish_date', datetime.fromtimestamp(video_info.get('pubdate', 0)).strftime('%Y-%m-%d')),
                "video_fallback": video_fallback
            }
            
            # ç”ŸæˆJSONæ–‡ä»¶è·¯å¾„ï¼ˆä¸MDæ–‡ä»¶åŒç›®å½•åŒåï¼‰
            md_path = Path(md_filepath)
            json_filepath = md_path.with_suffix('.json')
            
            # ä¿å­˜JSONæ–‡ä»¶
            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            return str(json_filepath)
            
        except Exception as e:
            self.logger.error(f"Failed to generate JSON file: {e}")
            # è¿”å›é»˜è®¤è·¯å¾„
            md_path = Path(md_filepath)
            return str(md_path.with_suffix('.json'))
    
    def send_email_report(self, bvid: str, to_email: str = None) -> bool:
        """å‘é€é‚®ä»¶æŠ¥å‘Š"""
        try:
            to_email = to_email or os.getenv('EMAIL_TO')
            if not to_email:
                self.logger.error("Recipient email not configured")
                return False

            # æ£€æŸ¥å¤„ç†è®°å½•
            processed = self._load_processed_videos()
            if bvid not in processed:
                self.logger.error(f"Video {bvid} not yet processed")
                return False

            md_path = processed[bvid].get('subtitle_path')
            if not md_path or not os.path.exists(md_path):
                self.logger.error(f"Processed document not found: {md_path}")
                return False
            
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.api.get_video_info(bvid)
            
            # è§£æMarkdownæ–‡ä»¶ç”ŸæˆHTMLé‚®ä»¶
            html_content = self._generate_email_html(md_path)
            
            # å‘é€é‚®ä»¶
            success = self.email_sender.send_video_report(
                to_email=to_email,
                video_title=video_info['title'],
                bvid=bvid,
                html_content=html_content,
                markdown_path=md_path
            )
            
            if success:
                self.logger.info(f"Email sent to {to_email}")
            else:
                self.logger.error("Failed to send email")

            return success

        except Exception as e:
            self.logger.error(f"Failed to send email: {e}")
            return False
    
    def _generate_email_html(self, md_path: str) -> str:
        """ä»Markdownæ–‡ä»¶ç”ŸæˆHTMLé‚®ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ç®€å•çš„Markdownåˆ°HTMLè½¬æ¢
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>æ©˜é¸¦AIæ—©æŠ¥</title>
</head>
<body style="font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px;">
    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 8px;">
        <pre style="white-space: pre-wrap; font-family: inherit;">{content}</pre>
    </div>
</body>
</html>
"""
        return html

    def process_history_reports(self, days: int = 30, force_regenerate: bool = False) -> Dict:
        """å¤„ç†å†å²AIæ—©æŠ¥"""
        self.logger.info(f"Starting history processing for {days} days...")

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = date.today() - timedelta(days=1)  # ä¸åŒ…æ‹¬ä»Šå¤©
        start_date = end_date - timedelta(days=days - 1)

        self.logger.info(f"Processing date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")

        # è·å–æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰AIæ—©æŠ¥è§†é¢‘
        ai_reports = self.get_ai_reports_by_date_range(start_date, end_date)

        if not ai_reports:
            self.logger.warning("No historical AI report videos found")
            return {
                'total_found': 0,
                'total_processed': 0,
                'total_skipped': 0,
                'total_failed': 0,
                'reports': []
            }

        # å¤„ç†æ¯ä¸ªè§†é¢‘
        results = []
        processed_count = 0
        skipped_count = 0
        failed_count = 0

        for report in ai_reports:
            bvid = report['bvid']
            title = report['title']
            report_date = report['date']

            self.logger.info(f"Processing video for {report_date}: {title} (BV: {bvid})")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ–‡æ¡£
            video_info = self.api.get_video_info(bvid)
            video_date = datetime.fromtimestamp(video_info['pubdate'])
            date_str = video_date.strftime('%Y-%m-%d')
            filename = f"{date_str}_AIæ—©æŠ¥_{bvid}.md"
            filepath = DOCS_DIR / filename

            if not force_regenerate and filepath.exists():
                self.logger.info(f"Document already exists, skipping: {filename}")
                skipped_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'skipped',
                    'reason': 'å·²å­˜åœ¨'
                })
                continue

            # å¤„ç†è§†é¢‘
            success = self.process_video(bvid, force_regenerate=force_regenerate)

            if success:
                processed_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'success',
                    'reason': 'å¤„ç†æˆåŠŸ'
                })
                self.logger.info("Processing completed successfully")
            else:
                failed_count += 1
                results.append({
                    'date': report_date,
                    'bvid': bvid,
                    'title': title,
                    'status': 'failed',
                    'reason': 'å¤„ç†å¤±è´¥'
                })
                self.logger.error("Processing failed")

        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.logger.info(f"History processing summary: found={len(ai_reports)}, processed={processed_count}, skipped={skipped_count}, failed={failed_count}")

        return {
            'total_found': len(ai_reports),
            'total_processed': processed_count,
            'total_skipped': skipped_count,
            'total_failed': failed_count,
            'date_range': {
                'start': start_date.strftime('%Y-%m-%d'),
                'end': end_date.strftime('%Y-%m-%d')
            },
            'reports': results
        }


def single_run(processor: JuyaProcessor, send_email: bool = False, generate_web: bool = False):
    """å•æ¬¡è¿è¡Œæ¨¡å¼ï¼šè·å–æœ€æ–°AIæ—©æŠ¥"""
    logger.info("="*60)
    logger.info("Single run mode - fetching latest AI report")
    logger.info("="*60)

    # è·å–æœ€æ–°AIæ—©æŠ¥è§†é¢‘
    bvid = processor.get_latest_ai_report()
    if not bvid:
        logger.error("No AI report video found")
        return False

    # å¤„ç†è§†é¢‘
    success = processor.process_video(bvid)
    if not success:
        logger.error("Failed to process video")
        return False

    # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if send_email:
        processor.send_email_report(bvid)

    # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if generate_web:
        logger.info("Generating static frontend website...")
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
        web_result = web_generator.generate_static_site()
        if web_result:
            logger.info("Static frontend website updated")
        else:
            logger.error("Failed to generate static frontend website")

    logger.info("Single run completed")
    return True


def bv_run(processor: JuyaProcessor, bvid: str, send_email: bool = False, generate_web: bool = False, force: bool = False):
    """æŒ‡å®šBVå·è¿è¡Œæ¨¡å¼"""
    logger.info("="*60)
    logger.info(f"Specified BV mode - {bvid}")
    logger.info("="*60)

    # å¤„ç†è§†é¢‘
    success = processor.process_video(bvid, force_regenerate=force)
    if not success:
        logger.error("Failed to process video")
        return False

    # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if send_email:
        processor.send_email_report(bvid)

    # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if generate_web:
        logger.info("Generating static frontend website...")
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
        web_result = web_generator.generate_static_site()
        if web_result:
            logger.info("Static frontend website updated")
        else:
            logger.error("Failed to generate static frontend website")

    logger.info("BV mode completed")
    return True


def loop_run(processor: JuyaProcessor, send_email: bool = False, generate_web: bool = False):
    """å®šæ—¶è¿è¡Œæ¨¡å¼ï¼šæ¯15åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡ï¼Œ0-7ç‚¹è·³è¿‡æ£€æŸ¥"""
    logger.info("="*60)
    logger.info("Scheduled run mode - checking every 15 minutes, skipping 0-7 hours")
    logger.info("="*60)

    if generate_web:
        logger.info("Auto frontend update mode enabled")

    check_interval = 900  # 15åˆ†é’Ÿ

    try:
        while True:
            current_time = datetime.now()
            current_hour = current_time.hour

            # æ£€æŸ¥æ˜¯å¦åœ¨è·³è¿‡æ—¶é—´ï¼ˆ0-7ç‚¹ï¼‰
            if 0 <= current_hour < 7:
                logger.info(f"Current time {current_time.strftime('%Y-%m-%d %H:%M:%S')} in skip period (0-7 hours), waiting {check_interval // 60} minutes")
                time.sleep(check_interval)
                continue

            logger.info(f"Starting check at {current_time.strftime('%Y-%m-%d %H:%M:%S')}")

            # æ£€æŸ¥ä»Šæ—¥æ˜¯å¦å·²æœ‰æŠ¥å‘Š
            if processor._check_today_report_exists():
                logger.info("Today's AI report already exists, skipping this check")
            else:
                # è·å–æœ€æ–°AIæ—©æŠ¥
                bvid = processor.get_latest_ai_report()
                if bvid:
                    # å¤„ç†è§†é¢‘
                    success = processor.process_video(bvid)
                    if success:
                        # å‘é€é‚®ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if send_email:
                            processor.send_email_report(bvid)

                        # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if generate_web:
                            logger.info("Updating static frontend website...")
                            web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
                            web_result = web_generator.generate_static_site()
                            if web_result:
                                logger.info("Static frontend website updated")
                            else:
                                logger.error("Failed to generate static frontend website")
                else:
                    logger.info("No new AI reports available")

            logger.info(f"Waiting {check_interval // 60} minutes for next check...")
            time.sleep(check_interval)

    except KeyboardInterrupt:
        logger.info("Scheduled run stopped")
    except Exception as e:
        logger.error(f"Scheduled run error: {e}")


def history_run(processor: JuyaProcessor, days: int = 30, force: bool = False, generate_web: bool = False):
    """å†å²è¿è¡Œæ¨¡å¼ï¼šå¤„ç†æŒ‡å®šå¤©æ•°çš„å†å²AIæ—©æŠ¥"""
    logger.info("="*60)
    logger.info(f"History run mode - processing {days} days of AI reports")
    logger.info("="*60)

    # å¤„ç†å†å²æŠ¥å‘Š
    result = processor.process_history_reports(days=days, force_regenerate=force)

    # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
    if result['total_found'] > 0:
        logger.info(f"History processing completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Date range: {result['date_range']['start']} to {result['date_range']['end']}")
        logger.info(f"Summary: found={result['total_found']}, processed={result['total_processed']}, skipped={result['total_skipped']}, failed={result['total_failed']}")

        # ä¿å­˜å¤„ç†æŠ¥å‘Š
        report_filename = f"history_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path = DOCS_DIR / report_filename

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"Detailed report saved: {report_path}")

        # ç”Ÿæˆé™æ€å‰ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if generate_web:
            logger.info("Generating static frontend website...")
            web_generator = WebGenerator(DOCS_DIR, DIST_DIR)
            web_result = web_generator.generate_static_site()
            if web_result:
                logger.info("Static frontend website updated")
            else:
                logger.error("Failed to generate static frontend website")
    else:
        logger.warning("No historical AI report videos found")

    return result


def web_run(processor: JuyaProcessor):
    """Webè¿è¡Œæ¨¡å¼ï¼šç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™"""
    logger.info("="*60)
    logger.info("Web run mode - generating static frontend website")
    logger.info("="*60)

    try:
        # åˆ›å»ºWebç”Ÿæˆå™¨
        web_generator = WebGenerator(DOCS_DIR, DIST_DIR)

        logger.info("ğŸ“ å‡†å¤‡ç”Ÿæˆé™æ€å‰ç«¯...")
        logger.info(f"   æºç›®å½•: {DOCS_DIR}")
        logger.info(f"   è¾“å‡ºç›®å½•: {DIST_DIR}")

        # ç”Ÿæˆé™æ€ç½‘ç«™
        result = web_generator.generate_static_site()

        if result:
            logger.info("Static frontend website generated successfully!")
            logger.info(f"Output directory: {DIST_DIR}")
            logger.info(f"Main page: {DIST_DIR}/index.html")
            logger.info("To view the website, open in browser:")
            logger.info(f"file://{DIST_DIR}/index.html")
        else:
            logger.error("Failed to generate static frontend website")
            return False

        return True

    except Exception as e:
        logger.error(f"Failed to generate static frontend: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Juya AIæ—©æŠ¥ç”Ÿæˆå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¿è¡Œç¤ºä¾‹:
  %(prog)s                           # å•æ¬¡è¿è¡Œï¼Œè·å–æœ€æ–°AIæ—©æŠ¥
  %(prog)s --single                  # åŒä¸Šï¼Œæ˜¾å¼æŒ‡å®šå•æ¬¡è¿è¡Œ
  %(prog)s --bv BV1234567890        # å¤„ç†æŒ‡å®šBVå·è§†é¢‘
  %(prog)s --loop                    # å®šæ—¶è¿è¡Œï¼Œæ¯15åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡ï¼Œ0-7ç‚¹è·³è¿‡æ£€æŸ¥
  %(prog)s --history                 # å¤„ç†å†å²30å¤©çš„AIæ—©æŠ¥
  %(prog)s --history 15              # å¤„ç†å†å²15å¤©çš„AIæ—©æŠ¥
  %(prog)s --history 30 --force      # å¼ºåˆ¶é‡æ–°ç”Ÿæˆå†å²30å¤©çš„AIæ—©æŠ¥
  %(prog)s --web                     # ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™åˆ°distç›®å½•

ç»„åˆé€‰é¡¹:
  %(prog)s --web                     # ä»…ç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™
  %(prog)s --single --web            # å•æ¬¡è¿è¡Œå¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --bv BV1234567890 --web   # å¤„ç†æŒ‡å®šBVå·å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --loop --web              # å®šæ—¶è¿è¡Œå¹¶è‡ªåŠ¨æ›´æ–°é™æ€å‰ç«¯
  %(prog)s --history --web           # å¤„ç†å†å²æ—©æŠ¥å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --send-email --web        # å‘é€é‚®ä»¶å¹¶ç”Ÿæˆé™æ€å‰ç«¯
  %(prog)s --history --force --web   # å¼ºåˆ¶é‡æ–°ç”Ÿæˆå†å²æ—©æŠ¥å¹¶æ›´æ–°é™æ€å‰ç«¯
        """
    )

    # è¿è¡Œæ¨¡å¼å‚æ•°ï¼ˆäº’æ–¥ï¼‰
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument('--single', action='store_true', help='å•æ¬¡è¿è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰')
    mode_group.add_argument('--bv', type=str, help='æŒ‡å®šBVå·è¿è¡Œæ¨¡å¼')
    mode_group.add_argument('--loop', action='store_true', help='å®šæ—¶è¿è¡Œæ¨¡å¼')
    mode_group.add_argument('--history', nargs='?', type=int, const=30, metavar='DAYS', help='å¤„ç†å†å²æŒ‡å®šå¤©æ•°çš„AIæ—©æŠ¥ï¼ˆé»˜è®¤30å¤©ï¼‰')

    # å…¶ä»–é€‰é¡¹
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²å­˜åœ¨çš„æ–‡æ¡£')
    parser.add_argument('--send-email', action='store_true', help='å¤„ç†å®Œæˆåå‘é€é‚®ä»¶')
    parser.add_argument('--web', action='store_true', help='å¤„ç†å®Œæˆåç”Ÿæˆé™æ€å‰ç«¯ç½‘ç«™ï¼ˆå¯ä¸å…¶ä»–å‚æ•°ç»„åˆä½¿ç”¨ï¼‰')

    args = parser.parse_args()

    # ç¡®å®šè¿è¡Œæ¨¡å¼
    if args.loop:
        mode = 'loop'
    elif args.bv:
        mode = 'bv'
    elif args.history is not None:
        mode = 'history'
    else:
        mode = 'single'  # é»˜è®¤æ¨¡å¼

    # åˆå§‹åŒ–å¤„ç†å™¨
    try:
        processor = JuyaProcessor()
    except Exception as e:
        logger.error(f"Initialization failed: {e}")
        sys.exit(1)

    # æ‰§è¡Œå¯¹åº”çš„è¿è¡Œæ¨¡å¼
    try:
        # å¦‚æœåªæœ‰--webå‚æ•°ï¼ˆæ²¡æœ‰å…¶ä»–ä»»ä½•å‚æ•°ï¼‰ï¼Œæ‰§è¡Œçº¯webç”Ÿæˆ
        if args.web and not args.single and not args.bv and not args.loop and args.history is None and not args.send_email and not args.force:
            web_run(processor)
        elif mode == 'single':
            single_run(processor, args.send_email, args.web)
        elif mode == 'bv':
            bv_run(processor, args.bv, args.send_email, args.web, args.force)
        elif mode == 'loop':
            loop_run(processor, args.send_email, args.web)
        elif mode == 'history':
            history_run(processor, days=args.history, force=args.force, generate_web=args.web)
    except KeyboardInterrupt:
        logger.info("Program stopped")
    except Exception as e:
        logger.error(f"Runtime error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
